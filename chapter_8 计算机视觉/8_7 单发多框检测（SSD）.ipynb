{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c52b04c",
   "metadata": {},
   "source": [
    "# 单发多框检测（SSD）\n",
    "在前面的章节中，我们介绍了边界框、锚框、多尺度目标检测和用于目标检测的数据集，现在我们来设计一个目标检测模型：单发多框检测（SSD）。该模型简单、快速且被广泛使用，尽管这只是其中一种目标检测模型，但本节中的一些设计原则和实现细节也使用于其他模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34f7fbd",
   "metadata": {},
   "source": [
    "## 模型\n",
    "![单发多框检测模型主要由一个基础网络块和若干多尺度特征块串联而成。](./ssd.svg)\n",
    ":label:`fig_ssd`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96509b31",
   "metadata": {},
   "source": [
    "上图描述了单发多框检测模型的设计。此模型主要由基础网络组成，其后是几个多尺度特征块。基础网络用于从输入图像中提取特征，因此它可以使用深度卷积神经网络。单发多框检测论文中选用了在分类层之前截断的VGG，现在也常用ResNet替代。我们可以设计基础网络，使它输出的宽和高较大，这样一来，基于该特征图生成的锚框数量较多，可以用来检测尺寸较小的目标。接下来的每个多尺度特征块将上一层提供的特征图的高和宽缩小（如减半），并使特征图中每个单元在输入图像中的感受野变得更广阔。\n",
    "\n",
    "**简而言之，通过多尺度特征块，单发多框检测生成不同大小的锚框，并通过预测边界框的类别和偏移量来检测大小不同的目标**，因此这是一个多尺度目标检测模型。\n",
    "\n",
    "下面，我们将介绍不同块的实现细节。首先，我们将讨论如何实施类别和边界框预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fc7180",
   "metadata": {},
   "source": [
    "### 类别检测层\n",
    "设目标类别的数量为$q$。这样一来，锚框有$q+1$个类别，其中0类是背景。在某个尺度下，设特征图的高和宽分别为$h$和$w$。如果以其中每个单元为中心生成$a$个锚框，那么我们需要对$hwa$个锚框进行分类。如果使用全连接层作为输出，很容易导致模型参数过多。\n",
    "\n",
    "因此，类别预测层使用一个保持输入高和宽的卷积层，这样一来，输出和输入在特征图宽和高上的空间坐标一一对应。考虑输出和输入同一空间坐标（$x$、$y$）：输出特征图上（$x$、$y$）坐标的通道里包含了以输入特征图（$x$、$y$）坐标为中心生成的所有锚框的类别预测。因此**输出通道数为$a(q+1)$，其中索引为$i(q+1) + j$（$0 \\leq j \\leq q$）的通道代表了索引为$i$的锚框有关类别索引为$j$的预测**。\n",
    "\n",
    "在下面，我们定义了这样一个类别预测层，通过参数`num_anchors`和`num_classes`分别指定了$a$和$q$。\n",
    "该图层使用填充为1的$3\\times3$的卷积层。此卷积层的输入和输出的宽度和高度保持不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3ffbe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c82f247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_predictor(num_inputs, num_anchors, num_classes):\n",
    "    return nn.Conv2d(num_inputs, num_anchors * (num_classes + 1), \n",
    "                    kernel_size=3, padding=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd73633",
   "metadata": {},
   "source": [
    "### 边界框预测层\n",
    "边界框预测层的设计与类别预测层的设计类似，唯一不同的是，这里需要为每个锚框预测4个偏移量，而不是q+1个类别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d1f7ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_predictor(num_inputs, num_anchors):\n",
    "    return nn.Conv2d(num_inputs, num_anchors * 4, \n",
    "                    kernel_size=3, padding=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999db51e",
   "metadata": {},
   "source": [
    "### 连结多尺度的预测\n",
    "单发多框检测使用多尺度特征图来生成锚框并预测其类别和偏移量。在不同尺度下，特征图的形状或以同一单元为中心的锚框的数量可能会有所不同。因此，不同尺度下预测输出的形状可能会有所不同。\n",
    "\n",
    "在以下示例中，我们为同一小批量构建两个不同比例（`Y1`和`Y2`）的特征图，其中`Y2`的高度和宽度是`Y1`的一半。以类别预测为例，假设`Y1`和`Y2`的每个单元分别生成了$5$个和$3$个锚框。进一步假设目标类别的数量为$10$，对于特征图`Y1`和`Y2`，类别预测输出中的通道数分别为$5\\times(10+1)=55$和$3\\times(10+1)=33$，其中任一输出的形状是（批量大小，通道数，高度，宽度）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1afb01cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 55, 20, 20]), torch.Size([2, 33, 10, 10]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forward(x, block):\n",
    "    return block(x)\n",
    "\n",
    "Y1 = forward(torch.zeros((2, 8, 20, 20)), cls_predictor(8, 5, 10))\n",
    "Y2 = forward(torch.zeros((2, 16, 10, 10)), cls_predictor(16, 3, 10))\n",
    "Y1.shape, Y2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f01f3d",
   "metadata": {},
   "source": [
    "正如我们所看到的，除了批量大小这一维度外，其他三个维度都具有不同的尺寸。 为了将这两个预测输出链接起来以提高计算效率，我们将把这些张量转换为更一致的格式。\n",
    "\n",
    "通道维包含中心相同的锚框的预测结果。我们首先将通道维移到最后一维。\n",
    "因为不同尺度下批量大小仍保持不变，我们可以将预测结果转成二维的（批量大小，高$\\times$宽$\\times$通道数）的格式，以方便之后在维度$1$上的连结。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07f17154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_pred(pred):\n",
    "    return torch.flatten(pred.permute(0, 2, 3, 1), start_dim=1)\n",
    "\n",
    "def concat_preds(preds):\n",
    "    return torch.cat([flatten_pred(pred) for pred in preds], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e61a7bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 25300])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_preds([Y1, Y2]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb4d928a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = torch.arange(24).reshape(2, 2, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb8d1b7",
   "metadata": {},
   "source": [
    "flatten_pred的作用是让同一个像素的框的类别预测结果连在一起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9546657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  6,  1,  7,  2,  8,  3,  9,  4, 10,  5, 11],\n",
       "        [12, 18, 13, 19, 14, 20, 15, 21, 16, 22, 17, 23]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_pred(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85319ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n",
       "        [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.flatten(x, start_dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209c5916",
   "metadata": {},
   "source": [
    "### 高和宽减半块\n",
    "为了在多个尺度下检测目标，我们在下面定义了高和宽减半块`down_sample_blk`，该模块将输入特征图的高度和宽度减半。事实上，该块应用了在 :numref:`subsec_vgg-blocks`中的VGG模块设计。\n",
    "更具体地说，每个高和宽减半块由两个填充为$1$的$3\\times3$的卷积层、以及步幅为$2$的$2\\times2$最大汇聚层组成。\n",
    "我们知道，填充为$1$的$3\\times3$卷积层不改变特征图的形状。但是，其后的$2\\times2$的最大汇聚层将输入特征图的高度和宽度减少了一半"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8003876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_sample_blk(in_channels, out_channels):\n",
    "    blk = []\n",
    "    for _ in range(2):\n",
    "        blk.append(nn.Conv2d(in_channels, out_channels,\n",
    "                             kernel_size=3, padding=1))\n",
    "        blk.append(nn.BatchNorm2d(out_channels))\n",
    "        blk.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "    blk.append(nn.MaxPool2d(2))\n",
    "    return nn.Sequential(*blk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e352303",
   "metadata": {},
   "source": [
    "在以下示例中，我们构建的高和宽减半块会更改输入通道的数量，并将输入特征图的高度和宽度减半。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f052dcce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 10, 10])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward(torch.zeros(2, 3, 20, 20), down_sample_blk(3, 10)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe87e5f",
   "metadata": {},
   "source": [
    "### 基本网络块\n",
    "基本网络块用于从输入图像中抽取特征。\n",
    "为了计算简洁，我们构造了一个小的基础网络，该网络串联3个高和宽减半块，并逐步将通道数翻倍。\n",
    "给定输入图像的形状为$256\\times256$，此基本网络块输出的特征图形状为$32 \\times 32$（$256/2^3=32$）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5da65c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_net():\n",
    "    blk = []\n",
    "    num_filters = [3, 16, 32, 64]\n",
    "    for i in range(3):\n",
    "        blk.append(down_sample_blk(num_filters[i], num_filters[i + 1]))\n",
    "    return nn.Sequential(*blk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "632bd3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 32, 32])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward(torch.zeros((2, 3, 256, 256)), base_net()).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ca205a",
   "metadata": {},
   "source": [
    "### 完整的模型\n",
    "完整的单发多框检测模型由五个模块组成，每个块生成的特征图既用于生成锚框，又用于预测这些锚框的类别和偏移量。在这五个模块中，第一个是基本网络块，第二个到第四个是高和宽减半块，最后一个模块使用全局最大池将高度和宽度都降到1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91b53db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blk(i):\n",
    "    if i == 0:\n",
    "        blk = base_net()\n",
    "    elif i == 1:\n",
    "        blk = down_sample_blk(64, 128)\n",
    "    elif i == 4:\n",
    "        blk = nn.AdaptiveMaxPool2d((1, 1))\n",
    "    else:\n",
    "        blk = down_sample_blk(128, 128)\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73160138",
   "metadata": {},
   "source": [
    "现在我们为每个块定义前向传播，与图像分类任务不同，此处的输出包括：CNN特征图`Y`；在当前尺度下根据`Y`生成的锚框；预测的这些锚框的类别和偏移量（基于`Y`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cbcffc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blk_forward(X, blk, size, ratio, cls_predictor, bbox_predictor):\n",
    "    Y = blk(X)\n",
    "    anchors = d2l.multibox_prior(Y, sizes=size, ratios=ratio)\n",
    "    cls_preds = cls_predictor(Y)\n",
    "    bbox_preds = bbox_predictor(Y)\n",
    "    return (Y, anchors, cls_preds, bbox_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8f9a80",
   "metadata": {},
   "source": [
    "超参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2047fb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [[0.2, 0.272], \n",
    "         [0.37, 0.447], \n",
    "         [0.54, 0.619], \n",
    "         [0.71, 0.79],\n",
    "         [0.88, 0.961]]\n",
    "ratios = [[1, 2, 0.5]] * 5\n",
    "num_anchors = len(sizes[0]) + len(ratios[0]) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d87c3b",
   "metadata": {},
   "source": [
    "现在，我们就可以按如下方式[**定义完整的模型**]`TinySSD`了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a04073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinySSD(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(TinySSD, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        idx_to_in_channels = [64, 128, 128, 128, 128]\n",
    "        for i in range(5):\n",
    "            # 即赋值语句self.blk_i=get_blk(i)\n",
    "            setattr(self, f'blk_{i}', get_blk(i))\n",
    "            setattr(self, f'cls_{i}', cls_predictor(idx_to_in_channels[i],\n",
    "                                                   num_anchors, num_classes))\n",
    "            setattr(self, f'bbox_{i}', bbox_predictor(idx_to_in_channels[i],\n",
    "                                                      num_anchors))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        anchors, cls_preds, bbox_preds = [None] * 5, [None] * 5, [None] * 5\n",
    "        for i in range(5):\n",
    "            # getattr(self,'blk_%d'%i)即访问self.blk_i\n",
    "            X, anchors[i], cls_preds[i], bbox_preds[i] = blk_forward(\n",
    "                X, getattr(self, f'blk_{i}'), sizes[i], ratios[i],\n",
    "                getattr(self, f'cls_{i}'), getattr(self, f'bbox_{i}'))\n",
    "        anchors = torch.cat(anchors, dim=1)\n",
    "        cls_preds = concat_preds(cls_preds)\n",
    "        cls_preds = cls_preds.reshape(\n",
    "            cls_preds.shape[0], -1, self.num_classes + 1)\n",
    "        bbox_preds = concat_preds(bbox_preds)\n",
    "        return anchors, cls_preds, bbox_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412ac7d5",
   "metadata": {},
   "source": [
    "我们[**创建一个模型实例，然后使用它**]对一个$256 \\times 256$像素的小批量图像`X`(**执行前向传播**)。\n",
    "\n",
    "如本节前面部分所示，第一个模块输出特征图的形状为$32 \\times 32$。\n",
    "回想一下，第二到第四个模块为高和宽减半块，第五个模块为全局汇聚层。\n",
    "由于以特征图的每个单元为中心有$4$个锚框生成，因此在所有五个尺度下，每个图像总共生成$(32^2 + 16^2 + 8^2 + 4^2 + 1)\\times 4 = 5444$个锚框。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f466ca1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output anchors: torch.Size([1, 5444, 4])\n",
      "output class preds: torch.Size([32, 5444, 2])\n",
      "output bbox preds: torch.Size([32, 21776])\n"
     ]
    }
   ],
   "source": [
    "net = TinySSD(num_classes=1)\n",
    "X = torch.zeros((32, 3, 256, 256))\n",
    "anchors, cls_preds, bbox_preds = net(X)\n",
    "\n",
    "print('output anchors:', anchors.shape)\n",
    "print('output class preds:', cls_preds.shape)\n",
    "print('output bbox preds:', bbox_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9095bbef",
   "metadata": {},
   "source": [
    "## 定义损失函数和评价函数\n",
    "目标检测有两种类型的损失：第一种有关锚框类别的损失，我们可以简单地复用之前图像分类问题里一直使用的交叉熵损失函数来计算；第二种有关正类锚框偏移量的损失，预测偏移量是一个回归问题，但是这里我们不使用均方误差，而是使用$L_1$范数损失，即预测值和真实值之差的绝对值。掩码变量`bbox_masks`令负类锚框和填充锚框不参与损失的计算。最后，我们将锚框类别和偏移量的损失相加，以获得模型的最终损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab662e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "bbox_loss = nn.L1Loss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3347cf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks):\n",
    "    batch_size, num_classes = cls_preds.shape[0], cls_pres.shape[2]\n",
    "    cls = cls_loss(cls_preds.reshape(-1, num_classes),\n",
    "                   cls_labels.reshape(-1)).reshape(batch_size, -1).mean(dim=1)\n",
    "    bbox = bbox_loss(bbox_preds * bbox_masks,\n",
    "                     bbox_labels * bbox_masks).mean(dim=1)\n",
    "    return cls + bbox    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df9c967",
   "metadata": {},
   "source": [
    "我们可以沿用准确率评价分类结果。\n",
    "由于偏移量使用了$L_1$范数损失，我们使用*平均绝对误差*来评价边界框的预测结果。这些预测结果是从生成的锚框及其预测偏移量中获得的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8f1ad0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_eval(cls_preds, cls_labels):\n",
    "    # 由于类别预测结果放在最后一维，argmax需要指定最后一维。\n",
    "    return float((cls_preds.argmax(dim=-1).type(\n",
    "        cls_labels.dtype) == cls_labels).sum())\n",
    "\n",
    "def bbox_eval(bbox_preds, bbox_labels, bbox_masks):\n",
    "    return float((torch.abs((bbox_labels - bbox_preds) * bbox_masks)).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d62d30",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51097e5",
   "metadata": {},
   "source": [
    "让我们[**读取**] :numref:`sec_object-detection-dataset`中描述的(**香蕉检测数据集**)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1802b51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ..\\data\\banana-detection.zip from http://d2l-data.s3-accelerate.amazonaws.com/banana-detection.zip...\n",
      "read 1000 training examples\n",
      "read 100 validation examples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_iter, _ = d2l.load_data_bananas(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332eaf95",
   "metadata": {},
   "source": [
    "## 定义模型和优化器\n",
    "香蕉检测数据集中，目标的类别数为1。\n",
    "定义好模型后，我们需要(**初始化其参数并定义优化算法**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9f54f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = TinySSD(num_classes=1)\n",
    "net.to(device)\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.2, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa788397",
   "metadata": {},
   "source": [
    "## 训练\n",
    "在训练模型时，我们需要在模型的前向传播过程中生成多尺度锚框（`anchors`），并预测其类别（`cls_preds`）和偏移量（`bbox_preds`）。\n",
    "然后，我们根据标签信息`Y`为生成的锚框标记类别（`cls_labels`）和偏移量（`bbox_labels`）。\n",
    "最后，我们根据类别和偏移量的预测和标注值计算损失函数。为了代码简洁，这里没有评价测试数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674ccb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    for features, target in train_iter:\n",
    "        trainer.zero_grad()\n",
    "        X, Y = features.to(device), target.to(device)\n",
    "        # 生成多尺度的锚框，为每个锚框预测类别和偏移量\n",
    "        anchors, cls_preds, bbox_preds = net(X)\n",
    "        # 为每个锚框标注类别和偏移量\n",
    "        bbox_labels, bbox_masks, cls_labels = d2l.multibox_target(anchors, Y)\n",
    "        # 根据类别和偏移量的预测和标注值计算损失函数\n",
    "        l = calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels,\n",
    "                      bbox_masks)\n",
    "        l.mean().backward()\n",
    "        trainer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7381365",
   "metadata": {},
   "source": [
    "## 预测\n",
    "在预测阶段，我们希望能把图像里面所有我们感兴趣的目标检测出来。在下面，我们读取并调整测试图像的大小，然后将其转成卷积层需要的四维格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7e4e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torchvision.io.read_image('./banana.jpg').unsqueeze(0).float()\n",
    "img = X.squeeze(0).permute(1, 2, 0).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a9c0d6",
   "metadata": {},
   "source": [
    "使用下面的`multibox_detection`函数，我们可以根据锚框及其预测偏移量得到预测边界框。然后，通过非极大值抑制来移除相似的预测边界框。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9451019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    net.eval()\n",
    "    anchors, cls_preds, bbox_preds = net(X.to(device))\n",
    "    cls_probs = F.softmax(cls_preds, dim=2).permute(0, 2, 1)\n",
    "    output = d2l.multibox_detection(cls_probs, bbox_preds, anchors)\n",
    "    idx = [i for i, row in enumerate(output[0]) if row[0] != -1]\n",
    "    return output[0, idx]\n",
    "\n",
    "output = predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5e06d9",
   "metadata": {},
   "source": [
    "最后，我们[**筛选所有置信度不低于0.9的边界框，做为最终输出**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c47947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(img, output, threshold):\n",
    "    d2l.set_figsize((5, 5))\n",
    "    fig = d2l.plt.imshow(img)\n",
    "    for row in output:\n",
    "        score = float(row[1])\n",
    "        if score < threshold:\n",
    "            continue\n",
    "        h, w = img.shape[0:2]\n",
    "        bbox = [row[2:6] * torch.tensor((w, h, w, h), device=row.device)]\n",
    "        d2l.show_bboxes(fig.axes, bbox, '%.2f' % score, 'w')\n",
    "\n",
    "display(img, output.cpu(), threshold=0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
