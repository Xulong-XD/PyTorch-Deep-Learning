{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d55e57c9",
   "metadata": {},
   "source": [
    "# 批量标准化\n",
    "## 训练深层网络\n",
    "训练神经网络时，我们会遇到一些常见的挑战：\n",
    "* 数据预处理的方式通常会对最终结果产生巨大影响，比如标准化输入特征，使其平均值为0，方差为1。 直观地说，这种标准化可以很好地与我们的优化器配合使用，因为它可以将参数的量级进行统一。\n",
    "* 对于典型的多层感知机或卷积神经网络。当我们训练时，中间层中的变量可能具有更广的变化范围：批量规范化的发明者非正式地假设，这些变量分布中的这种偏移可能会阻碍网络的收敛。 直观地说，我们可能会猜想，如果一个层的可变值是另一层的100倍，这可能需要对学习率进行补偿调整。\n",
    "* 更深层的网络很复杂，容易过拟合。 这意味着正则化变得更加重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83191d7",
   "metadata": {},
   "source": [
    "## 批标准化的原理\n",
    "批量标准化的原理如下：在每次训练迭代中，我们首先规范化输入，即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理。接下来，我们应用比例系数和比例偏移。正是由于这个基于批量统计的标准化，才有了批量规范化的名称。\n",
    "\n",
    "请注意，如果我们尝试使用大小为1的小批量应用批量规范化，我们将无法学到任何东西。 这是因为在减去均值之后，每个隐藏单元将为0。 所以，只有使用足够大的小批量，批量规范化这种方法才是有效且稳定的。 请注意，在应用批量规范化时，批量大小的选择可能比没有批量规范化时更重要。\n",
    "\n",
    "从形式上说，用$\\mathbf{x} \\in \\mathcal{B}$表示一个来自小批量$\\mathcal{B}$的输入，批量规范化$\\mathrm{BN}$根据以下表达式转换$\\mathbf{x}$：\n",
    "\n",
    "$$\\mathrm{BN}(\\mathbf{x}) = \\boldsymbol{\\gamma} \\odot \\frac{\\mathbf{x} - \\hat{\\boldsymbol{\\mu}}_\\mathcal{B}}{\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}} + \\boldsymbol{\\beta}.$$\n",
    ":eqlabel:`eq_batchnorm`\n",
    "\n",
    "在 :eqref:`eq_batchnorm`中，$\\hat{\\boldsymbol{\\mu}}_\\mathcal{B}$是小批量$\\mathcal{B}$的样本均值，$\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}$是小批量$\\mathcal{B}$的样本标准差。\n",
    "应用标准化后，生成的小批量的平均值为0和单位方差为1。\n",
    "由于单位方差（与其他一些魔法数）是一个主观的选择，因此我们通常包含\n",
    "*拉伸参数*（scale）$\\boldsymbol{\\gamma}$和*偏移参数*（shift）$\\boldsymbol{\\beta}$，它们的形状与$\\mathbf{x}$相同。\n",
    "请注意，$\\boldsymbol{\\gamma}$和$\\boldsymbol{\\beta}$是需要与其他模型参数一起学习的参数。\n",
    "\n",
    "由于在训练过程中，中间层的变化幅度不能过于剧烈，而批量规范化将每一层主动居中，并将它们重新调整为给定的平均值和大小（通过$\\hat{\\boldsymbol{\\mu}}_\\mathcal{B}$和${\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}}$）。\n",
    "\n",
    "从形式上来看，我们计算出 :eqref:`eq_batchnorm`中的$\\hat{\\boldsymbol{\\mu}}_\\mathcal{B}$和${\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}}$，如下所示：\n",
    "\n",
    "$$\\begin{aligned} \\hat{\\boldsymbol{\\mu}}_\\mathcal{B} &= \\frac{1}{|\\mathcal{B}|} \\sum_{\\mathbf{x} \\in \\mathcal{B}} \\mathbf{x},\\\\\n",
    "\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}^2 &= \\frac{1}{|\\mathcal{B}|} \\sum_{\\mathbf{x} \\in \\mathcal{B}} (\\mathbf{x} - \\hat{\\boldsymbol{\\mu}}_{\\mathcal{B}})^2 + \\epsilon.\\end{aligned}$$\n",
    "\n",
    "请注意，我们在方差估计值中添加一个小的常量$\\epsilon > 0$，以确保我们永远不会尝试除以零。另外，深度学习中一个反复出现的主题是优化中的各种噪声源通常会导致更快的训练和较少的过拟合，但尚未在理论上有明确的证明。\n",
    "\n",
    "在一些初步研究中， :cite:`Teye.Azizpour.Smith.2018`和 :cite:`Luo.Wang.Shao.ea.2018`分别将批量规范化的性质与贝叶斯先验相关联。这些理论揭示了为什么批量规范化最适应$50 \\sim 100$范围中的中等批量大小的难题。\n",
    "\n",
    "另外，批量规范化层在”训练模式“（通过小批量统计数据规范化）和“预测模式”（通过数据集统计规范化）中的功能不同。在训练过程中，我们无法得知使用整个数据集来估计平均值和方差，所以只能根据每个小批次的平均值和方差不断训练模型。而在预测模式下，可以根据整个数据集精确计算批量规范化所需的平均值和方差。\n",
    "\n",
    "现在，我们了解一下批量规范化在实践中是如何工作的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447e9ceb",
   "metadata": {},
   "source": [
    "## 批量规范化层（BN）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4332dc1",
   "metadata": {},
   "source": [
    "全连接层和卷积层，他们的批量规范化实现略有不同。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1ba678",
   "metadata": {},
   "source": [
    "### 全连接层\n",
    "通常，我们将批规范化层置于全连接层的仿射变换和激活函数之间。设全连接层的输入为$x$、权重参数和偏置参数分别是$\\mathbf{W}$和$\\mathbf{b}$，激活函数为$\\phi$，批量规范化的运算符为$\\mathrm{BN}$。那么，使用批量规范化的全连接层的输出的计算详情如下：\n",
    "\n",
    "$$\\mathbf{h} = \\phi(\\mathrm{BN}(\\mathbf{W}\\mathbf{x} + \\mathbf{b}) ).$$\n",
    "\n",
    "### 卷积层\n",
    "同样，对于卷积层，我们可以在卷积层之后和非线性激活函数之前应用批量规范化。\n",
    "当卷积有多个输出通道时，我们需要对这些通道的“每个”输出执行批量规范化，每个通道都有自己的拉伸（scale）和偏移（shift）参数，这两个参数都是标量。\n",
    "\n",
    "假设我们的小批量包含$m$个样本，并且对于每个通道，卷积的输出具有高度$p$和宽度$q$。\n",
    "那么对于卷积层，我们在每个输出通道的$m \\cdot p \\cdot q$个元素上同时执行每个批量规范化。\n",
    "因此，在计算平均值和方差时，我们会收集所有空间位置的值，然后在给定通道内应用相同的均值和方差，以便在每个空间位置对值进行规范化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879bbac8",
   "metadata": {},
   "source": [
    "### 预测过程中的批量规范化\n",
    "\n",
    "正如我们前面提到的，批量规范化在训练模式和预测模式下的行为通常不同。\n",
    "首先，将训练好的模型用于预测时，我们不再需要样本均值中的噪声以及在微批次上估计每个小批次产生的样本方差了。\n",
    "其次，例如，我们可能需要使用我们的模型对逐个样本进行预测。\n",
    "一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。\n",
    "可见，和暂退法一样，批量规范化层在训练模式和预测模式下的计算结果也是不一样的。\n",
    "\n",
    "## (**从零实现**)\n",
    "\n",
    "下面，我们从头开始实现一个具有张量的批量规范化层。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c2d1118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b0a5bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    # 通过is_grad_enabled来判断当前模式是训练模式还是测试模式\n",
    "    if not torch.is_grad_enabled():\n",
    "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4)\n",
    "        if len(X.shape) == 2:\n",
    "            # 使用全连接层的情况，计算特征维上的均值和方差\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X - mean) ** 2).mean(dim=0)\n",
    "        else:\n",
    "            # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。\n",
    "            # 这里我们需要保持X的形状以便后面可以做广播运算\n",
    "            mean = X.mean(dim=(0, 2, 3), keepdim=True)  # 计算每个通道上的均值\n",
    "            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)  # 计算每个通道上的方差\n",
    "        # 在训练模式下，用当前的均值和方差做标准化\n",
    "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
    "        # 更新移动平均的均值和方差\n",
    "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
    "        moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
    "    Y = gamma * X_hat + beta  # 缩放和移位\n",
    "    return Y, moving_mean.data, moving_var.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a861d2",
   "metadata": {},
   "source": [
    "我们现在可以[**创建一个正确的`BatchNorm`层**]。\n",
    "这个层将保持适当的参数：拉伸`gamma`和偏移`beta`,这两个参数将在训练过程中更新。\n",
    "此外，我们的层将保存均值和方差的移动平均值，以便在模型预测期间随后使用。\n",
    "\n",
    "撇开算法细节，注意我们实现层的基础设计模式。\n",
    "通常情况下，我们用一个单独的函数定义其数学原理，比如说`batch_norm`。\n",
    "然后，我们将此功能集成到一个自定义层中，其代码主要处理数据移动到训练设备（如GPU）、分配和初始化任何必需的变量、跟踪移动平均线（此处为均值和方差）等问题。\n",
    "为了方便起见，我们并不担心在这里自动推断输入形状，因此我们需要指定整个特征的数量。\n",
    "不用担心，深度学习框架中的批量规范化API将为我们解决上述问题，我们稍后将展示这一点。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5b25853",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    # num_features：全连接层的输出数量或卷积层的输出通道数。\n",
    "    # num_dims：2表示完全连接层，4表示卷积层\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super().__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化为1和0\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        # 非模型参数的变量初始化为0和1\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.ones(shape)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # 如果X不在内存上，将moving_mean和moving_var\n",
    "        # 复制到X所在显存上\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        # 保存更新过的moving_mean和moving_var\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
    "            X, self.gamma, self.beta, self.moving_mean,\n",
    "            self.moving_var, eps=1e-5, momentum=0.9)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b05351",
   "metadata": {},
   "source": [
    "##  使用批量规范化层的 LeNet\n",
    "\n",
    "为了更好理解如何[**应用`BatchNorm`**]，下面我们将其应用(**于LeNet模型**)（ :numref:`sec_lenet`）。\n",
    "回想一下，批量规范化是在卷积层或全连接层之后、相应的激活函数之前应用的。\n",
    "\n",
    "### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d98f937",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, kernel_size=5), BatchNorm(6, num_dims=4), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(6, 16, kernel_size=5), BatchNorm(16, num_dims=4), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),\n",
    "    nn.Linear(16*4*4, 120), BatchNorm(120, num_dims=2), nn.Sigmoid(),\n",
    "    nn.Linear(120, 84), BatchNorm(84, num_dims=2), nn.Sigmoid(),\n",
    "    nn.Linear(84, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe1ea6ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (1): BatchNorm()\n",
       "  (2): Sigmoid()\n",
       "  (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (5): BatchNorm()\n",
       "  (6): Sigmoid()\n",
       "  (7): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (8): Flatten(start_dim=1, end_dim=-1)\n",
       "  (9): Linear(in_features=256, out_features=120, bias=True)\n",
       "  (10): BatchNorm()\n",
       "  (11): Sigmoid()\n",
       "  (12): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (13): BatchNorm()\n",
       "  (14): Sigmoid()\n",
       "  (15): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36463dfa",
   "metadata": {},
   "source": [
    "### 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f2e613d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tran = transforms.ToTensor()\n",
    "test_data = torchvision.datasets.FashionMNIST(root='../data', \n",
    "                                               train=False, \n",
    "                                               download=True, \n",
    "                                               transform=tran)\n",
    "\n",
    "train_data = torchvision.datasets.FashionMNIST(root='../data', \n",
    "                                               train=True, \n",
    "                                               download=True, \n",
    "                                               transform=tran)                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c9cba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter = data.DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_iter = data.DataLoader(test_data, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b225240f",
   "metadata": {},
   "source": [
    "### 定义损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc65f97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaef890",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net.to(device)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d740e777",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b64b5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy in epoch 1: 0.7618666887283325\n",
      "accuracy in epoch 2: 0.8391166925430298\n",
      "accuracy in epoch 3: 0.8567500114440918\n",
      "accuracy in epoch 4: 0.8690166473388672\n",
      "accuracy in epoch 5: 0.875\n",
      "accuracy in epoch 6: 0.8811166882514954\n",
      "accuracy in epoch 7: 0.8862000107765198\n",
      "accuracy in epoch 8: 0.8903999924659729\n",
      "accuracy in epoch 9: 0.895550012588501\n",
      "accuracy in epoch 10: 0.8988333344459534\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    correct = 0\n",
    "    for X, y in train_iter:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)  # 计算的是这个batch中所有样本的平均交叉熵损失\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        # _,  pred = torch.max(y_hat.data, 1)\n",
    "        # correct += torch.sum(pred == y)\n",
    "        correct += (torch.argmax(y_hat, axis=1) == y).sum()\n",
    "    print(f'accuracy in epoch {epoch + 1}: {correct / len(train_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28385c24",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc1e2fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on testing dataset: 0.8729999661445618\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    for X, y in test_iter:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_hat = net(X)\n",
    "        correct += (torch.argmax(y_hat, axis=1) == y).sum()\n",
    "    print(f'accuracy on testing dataset: {correct / len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3c1e04",
   "metadata": {},
   "source": [
    "## 简明实现\n",
    "除了使用我们刚刚定义的`BatchNorm`，我们也可以直接使用深度学习框架中定义的`BatchNorm2d`和`BatchNorm1d`。\n",
    "该代码看起来几乎与我们上面的代码相同。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2359dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, kernel_size=5), nn.BatchNorm2d(6), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(6, 16, kernel_size=5), nn.BatchNorm2d(16), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),\n",
    "    nn.Linear(256, 120), nn.BatchNorm1d(120), nn.Sigmoid(),\n",
    "    nn.Linear(120, 84), nn.BatchNorm1d(84), nn.Sigmoid(),\n",
    "    nn.Linear(84, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe5af6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): Sigmoid()\n",
       "  (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): Sigmoid()\n",
       "  (7): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (8): Flatten(start_dim=1, end_dim=-1)\n",
       "  (9): Linear(in_features=256, out_features=120, bias=True)\n",
       "  (10): BatchNorm1d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): Sigmoid()\n",
       "  (12): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (13): BatchNorm1d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): Sigmoid()\n",
       "  (15): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690b4c75",
   "metadata": {},
   "source": [
    "## 小结\n",
    "* 在模型训练过程中，批量规范化利用小批量的均值和标准差，不断调整神经网络的中间输出，使整个神经网络各层的中间输出值更加稳定。\n",
    "* 批量规范化在全连接层和卷积层的使用略有不同。\n",
    "* 批量规范化层和暂退层一样，在训练模式和预测模式下计算不同。\n",
    "* 批量规范化有许多有益的副作用，主要是正则化。另一方面，”减少内部协变量偏移“的原始动机似乎不是一个有效的解释。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97289bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
