{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eae4cc6b",
   "metadata": {},
   "source": [
    "# ResNet\n",
    "## 函数类\n",
    "首先，假设有一类特定的神经网络架构$\\mathcal{F}$，它包括学习速率和其他超参数设置。对于所有$f \\in \\mathcal{F}$，存在一些参数集（例如权重和偏置），这些参数可以通过在合适的数据集上进行训练而获得。现在假设$f^*$是我们真正想要找到的函数，如果是$f^* \\in \\mathcal{F}$，那我们可以轻而易举的训练得到它，但通常我们不会那么幸运。\n",
    "\n",
    "那么，怎样得到更近似真正$f^*$的函数呢？唯一合理的可能性是，我们需要设计一个更强大的架构$\\mathcal{F}'$。换句话说，我们预计$f^*_{\\mathcal{F}'}$比$f^*_{\\mathcal{F}}$“更近似”。然而，如果$\\mathcal{F} \\not\\subseteq \\mathcal{F}'$，则无法保证新的体系“更近似”。事实上，$f^*_{\\mathcal{F}'}$可能更糟：如 :numref:`fig_functionclasses`所示，对于非嵌套函数（non-nested function）类，较复杂的函数类并不总是向“真”函数$f^*$靠拢（复杂度由$\\mathcal{F}_1$向$\\mathcal{F}_6$递增）。在 :numref:`fig_functionclasses`的左边，虽然$\\mathcal{F}_3$比$\\mathcal{F}_1$更接近$f^*$，但$\\mathcal{F}_6$却离的更远了。相反对于 :numref:`fig_functionclasses`右侧的嵌套函数（nested function）类$\\mathcal{F}_1 \\subseteq \\ldots \\subseteq \\mathcal{F}_6$，我们可以避免上述问题。\n",
    "\n",
    "![对于非嵌套函数类，较复杂（由较大区域表示）的函数类不能保证更接近“真”函数（ $f^*$ ）。这种现象在嵌套函数类中不会发生。](./functionclasses.svg)\n",
    ":label:`fig_functionclasses`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278c6570",
   "metadata": {},
   "source": [
    "因此，只有当较复杂的函数类包含较小的函数类时，我们才能确保提高它们的性能。针对这一问题，何恺明等人提出了*残差网络*（ResNet） 。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ab5430",
   "metadata": {},
   "source": [
    "## 残差块\n",
    "让我们聚焦于神经网络局部：如图 :numref:`fig_residual_block`所示，假设我们的原始输入为$x$，而希望学出的理想映射为$f(\\mathbf{x})$（作为 :numref:`fig_residual_block`上方激活函数的输入）。\n",
    " :numref:`fig_residual_block`左图虚线框中的部分需要直接拟合出该映射$f(\\mathbf{x})$，而右图虚线框中的部分则需要拟合出残差映射$f(\\mathbf{x}) - \\mathbf{x}$。\n",
    "残差映射在现实中往往更容易优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5757c0",
   "metadata": {},
   "source": [
    "我们希望添加了虚线框内的模型之后可以在一定条件下包含原来的模型，即附加层包含原始函数作为其元素之一。以本节开头提到的恒等映射作为我们希望学出的理想映射$f(\\mathbf{x})$，我们只需将 :numref:`fig_residual_block`中右图虚线框内上方的加权运算（如仿射）的权重和偏置参数设成0，那么$f(\\mathbf{x})$即为恒等映射。\n",
    "\n",
    "![一个正常块（左图）和一个残差块（右图）。](./residual-block.svg)\n",
    ":label:`fig_residual_block`\n",
    "\n",
    "\n",
    "残差块的实现如下，下面的代码可以生成两种类型的网络：一种是当`use_1x1conv=False`时，应用ReLU非线性函数之前，将输入添加到输出。\n",
    "另一种是当`use_1x1conv=True`时，添加通过$1 \\times 1$卷积调整通道和分辨率。\n",
    "\n",
    "![包含以及不包含 $1 \\times 1$ 卷积层的残差块。](./resnet-block.svg)\n",
    ":label:`fig_resnet_block`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8cde880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81785d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, input_channels, num_channels, use_1x1conv=False, strides=1):\n",
    "        super(Residual, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_channels, \n",
    "                               kernel_size=3, stride=strides, padding=1)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels, \n",
    "                               kernel_size=3, stride=1, padding=1)\n",
    "        # 注意，输出尺寸是由参数strides控制的，\n",
    "        # 因为第一个3*3卷积和1*1卷积使用了该参数，\n",
    "        # 而第二个3*3卷积是保持尺寸不变的\n",
    "        if use_1x1conv == True:\n",
    "            self.conv3 = nn.Conv2d(input_channels, num_channels, \n",
    "                               kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = torch.relu(self.bn1(self.conv1(x)))\n",
    "        y = self.bn2(self.conv2(y))\n",
    "        if self.conv3:\n",
    "            x = self.conv3(x)\n",
    "        y += x\n",
    "        return torch.relu(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8737ff82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 6, 6])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = Residual(3,3)\n",
    "X = torch.rand(4, 3, 6, 6)\n",
    "Y = blk(X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "becddb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 6, 6])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Y +X).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61afbee9",
   "metadata": {},
   "source": [
    "我们也可以在[**增加输出通道数的同时，减半输出的高和宽**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1acd3bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 6, 3, 3]), torch.Size([4, 3, 6, 6]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = Residual(3,6, use_1x1conv=True, strides=2)\n",
    "blk(X).shape, X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73646661",
   "metadata": {},
   "source": [
    "## ResNet模型\n",
    " :numref:`fig_resnet18`描述了完整的ResNet-18。因为每个模块有4个卷积层（不包括恒等映射的$1\\times 1$卷积层）。加上第一个$7\\times 7$卷积层和最后一个全连接层，共有18层。\n",
    "\n",
    "![ResNet-18 架构](./resnet18.svg)\n",
    ":label:`fig_resnet18`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a44c5c",
   "metadata": {},
   "source": [
    "ResNet的前两层跟之前介绍的GoogLeNet中的一样：\n",
    "在输出通道数为64、步幅为2的$7 \\times 7$卷积层后，接步幅为2的$3 \\times 3$的最大汇聚层。\n",
    "不同之处在于ResNet每个卷积层后增加了批量规范化层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24f3337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3), \n",
    "                   nn.BatchNorm2d(64),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9939e0",
   "metadata": {},
   "source": [
    "GoogLeNet在后面接了4个由Inception块组成的模块。\n",
    "ResNet则使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。\n",
    "第一个模块的通道数同输入通道数一致。\n",
    "由于之前已经使用了步幅为2的最大汇聚层，所以无须减小高和宽。\n",
    "之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。\n",
    "\n",
    "下面我们来实现这个模块。注意，我们对第一个模块做了特别处理。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70e1a991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_block(input_channels, num_channels, num_residuals, first_block=False):\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            blk.append(Residual(input_channels, num_channels, \n",
    "                                use_1x1conv=True, strides=2))\n",
    "        else:\n",
    "            blk.append(Residual(num_channels, num_channels))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695f7cdc",
   "metadata": {},
   "source": [
    "接着在ResNet加入所有残差块，这里每个模块使用2个残差块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49596b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))\n",
    "b3 = nn.Sequential(*resnet_block(64, 128, 2))\n",
    "b4 = nn.Sequential(*resnet_block(128, 256, 2))\n",
    "b5 = nn.Sequential(*resnet_block(256, 512, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c814adfe",
   "metadata": {},
   "source": [
    "最后，与GoogLeNet一样，在ResNet中加入全局平均汇聚层，以及全连接层输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa7e4bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(b1, b2, b3, b4, b5, \n",
    "                    nn.AdaptiveAvgPool2d((1,1)),\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(512, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dadef3",
   "metadata": {},
   "source": [
    "让我们[**观察一下ResNet中不同模块的输入形状是如何变化的**]。\n",
    "在之前所有架构中，分辨率降低，通道数量增加，直到全局平均汇聚层聚集所有特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd25648a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 64, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 128, 28, 28])\n",
      "Sequential output shape:\t torch.Size([1, 256, 14, 14])\n",
      "Sequential output shape:\t torch.Size([1, 512, 7, 7])\n",
      "AdaptiveAvgPool2d output shape:\t torch.Size([1, 512, 1, 1])\n",
      "Flatten output shape:\t torch.Size([1, 512])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(size=(1, 1, 224, 224))\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b1d1d3",
   "metadata": {},
   "source": [
    "## 小结\n",
    "ResNet的关键点在于，残差块允许神经网络在训练的时候选择不使用附加层，也就是当我们把前一个块的输出传入到残差块的附加层时，效果并不很好，那么这时候可能迭代器就不会去更新附加层中的权重和偏置参数，使它们趋于零，而只是直接把输入直接传到下一个块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d1cc05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
