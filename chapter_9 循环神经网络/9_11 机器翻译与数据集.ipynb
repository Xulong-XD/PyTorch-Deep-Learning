{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a935698b",
   "metadata": {},
   "source": [
    "# 机器翻译与数据集\n",
    "## 机器翻译\n",
    "语言模型是自然语言处理的关键，而机器翻译是语言模型最成功的基准测试。因为机器翻译正是将输入序列转换成输出序列的*序列转换模型*（sequence transduction）的核心问题。序列转换模型在各类现代人工智能应用中发挥着至关重要的作用。本节将介绍机器翻译问题及其后文需要使用的数据集。\n",
    "\n",
    "*机器翻译*（machine translation）指的是将序列从一种语言自动翻译成另一种语言。在使用神经网络进行端到端学习的兴起之前，统计学方法在这一领域一直占据主导地位。因为*统计机器翻译（statistical machine translation）涉及了翻译模型和语言模型等组成部分的统计分析，因此基于神经网络的方法通常被称为*神经机器翻译*（neuralmachine translation），用于将两种翻译模型区分开来。\n",
    "\n",
    "本书的关注点是神经网络机器翻译方法，强调的是端到端的学习。与 :numref:`sec_language_model`中的语料库是单一语言的语言模型问题存在不同，机器翻译的数据集是由源语言和目标语言的文本序列对组成的。因此，我们需要一种完全不同的方法来预处理机器翻译数据集，而不是复用语言模型的预处理程序。下面，我们看一下如何将预处理后的数据加载到小批量中用于训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecde6360",
   "metadata": {},
   "source": [
    "## 下载和预处理数据集\n",
    "数据集下载地址：http://d2l-data.s3-accelerate.amazonaws.com/fra-eng.zip\n",
    "\n",
    "组成的“英－法”数据集，数据集中的每一行都是制表符分隔的文本序列对，序列对由英文文本序列和翻译后的法语文本序列组成。请注意，每个文本序列可以是一个句子，也可以是包含多个句子的一个段落。在这个将英语翻译成法语的机器翻译问题中，英语是*源语言*（source language），法语是*目标语言*（target language）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f57df047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from d2l import torch as d2l\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43195533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_nmt():\n",
    "    \"\"\"载入英语-法语数据集\"\"\"\n",
    "    data_dir = 'C:/Users/lenovo/pytorch-learning/data/fra-eng'\n",
    "    with open(os.path.join(data_dir, 'fra.txt'), 'r',\n",
    "             encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "raw_text = read_data_nmt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a5e3310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tVa !\n",
      "Hi.\tSalut !\n",
      "Run!\tCours !\n",
      "Run!\tCourez !\n",
      "Who?\tQui ?\n",
      "Wow!\tÇa alors !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(raw_text[:75])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f775f9f6",
   "metadata": {},
   "source": [
    "下载数据集后，原始文本数据需要经过几个预处理步骤。例如，我们用空格代替*不间断空格*（non-breaking space），使用小写字母替换大写字母，并在单词和标点符号之间插入空格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "520e1cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_nmt(text):\n",
    "    \"\"\"预处理“英语－法语”数据集\"\"\"\n",
    "    def no_space(char, prev_char):\n",
    "        return char in set(',.!?') and prev_char != ' '\n",
    "\n",
    "    # 使用空格替换不间断空格\n",
    "    # 使用小写字母替换大写字母\n",
    "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ').lower()\n",
    "    # 在单词和标点符号之间插入空格\n",
    "    out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n",
    "           for i, char in enumerate(text)]\n",
    "    return ''.join(out)\n",
    "\n",
    "text = preprocess_nmt(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "16bddc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go .\tva !\n",
      "hi .\tsalut !\n",
      "run !\tcours !\n",
      "run !\tcourez !\n",
      "who ?\tqui ?\n",
      "wow !\t\n"
     ]
    }
   ],
   "source": [
    "print(text[:70])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4855a8c1",
   "metadata": {},
   "source": [
    "## 词元化\n",
    "和语言模型不一样，机器翻译中我们更常用单词级词元化。下面`tokenize_nmt`函数对前`num_examples`个文本序列对进行词元化，其中每个词元要么是一个词，要么是一个标点符号。此函数返回两个词元列表：`source`和`target`：`source[i]`是源语言（这里是英语）第$i$个文本序列的词元列表，`target[i]`是目标语言（这里是法语）第$i$个文本序列的词元列表。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f9130ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_nmt(text, num_examples=None):\n",
    "    \"\"\"词元化“英语－法语”数据数据集\"\"\"\n",
    "    source, target = [], []\n",
    "    for i, line in enumerate(text.split('\\n')):\n",
    "        if num_examples and i > num_examples:\n",
    "            break\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) == 2:\n",
    "            source.append(parts[0].split(' '))\n",
    "            target.append(parts[1].split(' '))\n",
    "    return source, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b046d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "source, target = tokenize_nmt(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc4b5cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['go', '.'],\n",
       "  ['hi', '.'],\n",
       "  ['run', '!'],\n",
       "  ['run', '!'],\n",
       "  ['who', '?'],\n",
       "  ['wow', '!']],\n",
       " [['va', '!'],\n",
       "  ['salut', '!'],\n",
       "  ['cours', '!'],\n",
       "  ['courez', '!'],\n",
       "  ['qui', '?'],\n",
       "  ['ça', 'alors', '!']])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source[:6], target[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2300c7c2",
   "metadata": {},
   "source": [
    "## 词表\n",
    "由于机器翻译数据集由语言对组成，因此我们可以分别为源语言和目标语言构建两个词表。使用单词级词元化时，词表大小将明显大于使用字符级词元化时的词表大小。为了缓解这一问题，这里我们将出现次数少于2次的低频率词元视为相同的未知（“&lt;unk&gt;”）词元。除此之外，我们还指定了额外的特定词元，例如在小批量时用于将序列填充到相同长度的填充词元（“&lt;pad&gt;”），以及序列的开始词元（“&lt;bos&gt;”）和结束词元（“&lt;eos&gt;”）。这些特殊词元在自然语言处理任务中比较常用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "629d27b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"文本词表\"\"\"\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens == None:\n",
    "            tokens = []\n",
    "        if reserved_tokens == None:\n",
    "            reserved_tokens = []\n",
    "        # 按出现频率排序\n",
    "        counter = self.count_corpus(tokens)\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                 reverse=True)\n",
    "        \n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "        for token, freq in self.token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "    \n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "    \n",
    "    # 统计词元的频率\n",
    "    def count_corpus(self, tokens):\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "        return collections.Counter(tokens)\n",
    "    \n",
    "    @property\n",
    "    def unk(self):  # 未知词元的索引为0\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3c527974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10012"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab = Vocab(source, min_freq=2,\n",
    "                      reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "len(src_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d25c7d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17851"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_vocab = Vocab(target, min_freq=2,\n",
    "                 reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "len(tgt_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc395366",
   "metadata": {},
   "source": [
    "## 加载数据集\n",
    "回想一下，语言模型中的序列样本都有一个固定的长度，这个固定的长度是由参数`num_steps`指定的。在机器翻译中，每个样本都是由源和目标组成的文本序列对，其中的每个文本序列可能具有不同的长度。为了提高计算效率，我们可以通过截断和填充的方式实现一次处理一个小批量的文本序列。\n",
    "\n",
    "假设同一个小批量中的每个序列都应该具有相同的长度`num_steps`，那么如果文本序列中的词元数目少于`num_steps`，我们将继续在其末尾添加特定的“&lt;pad&gt;”词元，直到其长度达到`num_steps`；反之，我们将截断文本序列，只取其前`num_steps`个词元，丢弃剩余的词元。这样，每个文本序列具有相同的长度，以便以相同形状的小批量进行加载。\n",
    "\n",
    "如前所述，下面的`truncate_pad`函数用来截断或填充文本序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5baa16e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_pad(line, num_steps, padding_token):\n",
    "    \"\"\"截断或填充文本序列\"\"\"\n",
    "    if len(line) > num_steps:\n",
    "        return line[:num_steps]  # 截断\n",
    "    return line + [padding_token] * (num_steps - len(line))  # 填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "27557ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[47, 4, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truncate_pad(src_vocab[source[0]], 10, src_vocab['<pad>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f35554",
   "metadata": {},
   "source": [
    "现在我们定义一个函数，可以将文本序列转换成小批量数据集用于训练。我们将特定的“&lt;eos&gt;”词元添加到所有序列的末尾，用于表示序列的结束。当模型通过一个词元接一个词元地生成序列进行预测时，生成的“&lt;eos&gt;”词元说明完成了序列输出工作。此外，我们还记录了每个文本序列的长度，统计长度时排除了填充词元，在稍后将要介绍的一些模型会需要这个长度信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e7ad72e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_array_nmt(lines, vocab, num_steps):\n",
    "    \"\"\"将机器翻译的文本序列转换成小批量\"\"\"\n",
    "    lines = [vocab[l] for l in lines]\n",
    "    lines = [l + [vocab['<eos>']] for l in lines]\n",
    "    array = torch.tensor([truncate_pad(\n",
    "        l, num_steps, vocab['<pad>']) for l in lines])\n",
    "    valid_len = (array != vocab['<pad>']).type(torch.int32).sum(1)\n",
    "    return array, valid_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79c24ba",
   "metadata": {},
   "source": [
    "最后，我们定义`load_data_nmt`函数来返回数据迭代器，以及源语言和目标语言的两种词表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8113d4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_nmt(batch_size, num_steps, num_examples=600):\n",
    "    \"\"\"返回翻译数据集的迭代器和词表\"\"\"\n",
    "    text = preprocess_nmt(read_data_nmt())\n",
    "    source, target = tokenize_nmt(text, num_examples)\n",
    "    src_vocab = Vocab(source, min_freq=2,\n",
    "                          reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "    tgt_vocab = Vocab(target, min_freq=2,\n",
    "                          reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)\n",
    "    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)\n",
    "    data_array = (src_array, src_valid_len, tgt_array, tgt_valid_len)\n",
    "    data_iter = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(*data_array),\n",
    "                                            batch_size, shuffle=True)\n",
    "    return data_iter, src_vocab, tgt_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7725a483",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data_nmt_results = load_data_nmt(batch_size=2, num_steps=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a5919f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(load_data_nmt_results, 'load_data_nmt_results.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1df361",
   "metadata": {},
   "source": [
    "## 小结\n",
    "* 机器翻译指的是将文本序列从一种语言自动翻译成另一种语言。\n",
    "* 使用单词级词元化时的词表大小，将明显大于使用字符级词元化时的词表大小。为了缓解这一问题，我们可以将低频词元视为相同的未知词元。\n",
    "* 通过截断和填充文本序列，可以保证所有的文本序列都具有相同的长度，以便以小批量的方式加载。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
