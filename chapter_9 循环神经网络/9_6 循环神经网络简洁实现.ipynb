{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ddb8c39",
   "metadata": {},
   "source": [
    "# 循环神经网络的简洁实现\n",
    "本节将展示如何使用深度学习框架的高级API提供的函数更有效地实现相同的语言模型。我们从读取时光机器数据集开始。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214cb097",
   "metadata": {},
   "source": [
    "## 读取序列数据\n",
    "首先解决如何构建数据迭代器的问题，即随机生成一个小批量数据的特征和标签。由于文本序列可以是任意长的，例如整本《时光机器》（*The Time Machine*）。因此我们可以将文本序列划分为具有相同步数的子序列，从而在训练模型时，这样的子序列就可以分批输入。假设网络一次只处理具有$n$个时间步的子序列。下图画出了从原始文本序列获得子序列的所有不同的方式，其中$n=5$，并且每个时间步的词元对应于一个字符。\n",
    "\n",
    "\n",
    "![分割文本时，不同的偏移量会导致不同的子序列](./timemachine-5gram.svg)\n",
    "\n",
    "然而，如果我们只选择一个偏移量，那么用于训练网络的、所有可能的子序列的覆盖范围将是有限的。因此，我们可以从随机偏移量开始划分序列，以同时获得*覆盖性*（coverage）和*随机性*（randomness）。下面，我们将描述如何实现*随机采样*（random sampling）和*顺序分区*（sequential partitioning）策略。\n",
    "\n",
    "### 随机采样\n",
    "在随机采样中，每个样本都是在原始的长序列上任意捕获的子序列。在迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列中相邻。对于语言模型，目标是基于到目前为止我们看到的词元来预测下一个词元，因此标签是移位了一个词元的原始序列。下面的代码每次可以从数据中随机生成一个小批量。在这里，参数`batch_size`指定了每个小批量中子序列样本的数目，参数`num_steps`是每个子序列中预定义的时间步数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "7cd18018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "0c8fd05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_data_iter_random(corpus, batch_size, num_steps):\n",
    "    \"\"\"使用随机抽样生成一个小批量子序列\"\"\"\n",
    "    # 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1\n",
    "    corpus = corpus[random.randint(0, num_steps - 1):]\n",
    "    # 减去1，是因为我们需要考虑标签\n",
    "    num_subseqs = (len(corpus) - 1) // num_steps\n",
    "    # 长度为num_steps的子序列的起始索引\n",
    "    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
    "    # 在随机抽样的迭代过程中，\n",
    "    # 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻\n",
    "    random.shuffle(initial_indices)\n",
    "\n",
    "    def data(pos):\n",
    "        # 返回从pos位置开始的长度为num_steps的序列\n",
    "        return corpus[pos: pos + num_steps]\n",
    "\n",
    "    num_batches = num_subseqs // batch_size  # \n",
    "    for i in range(0, batch_size * num_batches, batch_size):\n",
    "        # 在这里，initial_indices包含子序列的随机起始索引\n",
    "        initial_indices_per_batch = initial_indices[i: i + batch_size]\n",
    "        X = [data(j) for j in initial_indices_per_batch]\n",
    "        # X = [corpus[pos: pos + num_steps] for pos in initial_indices_per_batch]\n",
    "                \n",
    "        Y = [data(j + 1) for j in initial_indices_per_batch]\n",
    "        yield torch.tensor(X), torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47084920",
   "metadata": {},
   "source": [
    "下面我们[**生成一个从$0$到$34$的序列**]。假设批量大小为$2$，时间步数为$5$，这意味着可以生成$\\lfloor (35 - 1) / 5 \\rfloor= 6$个“特征－标签”子序列对。如果设置小批量大小为$2$，我们只能得到$3$个小批量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "759eabb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  tensor([[15, 16, 17, 18, 19],\n",
      "        [20, 21, 22, 23, 24]]) \n",
      "Y: tensor([[16, 17, 18, 19, 20],\n",
      "        [21, 22, 23, 24, 25]])\n",
      "X:  tensor([[ 0,  1,  2,  3,  4],\n",
      "        [25, 26, 27, 28, 29]]) \n",
      "Y: tensor([[ 1,  2,  3,  4,  5],\n",
      "        [26, 27, 28, 29, 30]])\n",
      "X:  tensor([[ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14]]) \n",
      "Y: tensor([[ 6,  7,  8,  9, 10],\n",
      "        [11, 12, 13, 14, 15]])\n"
     ]
    }
   ],
   "source": [
    "my_seq = list(range(35))\n",
    "for X, Y in seq_data_iter_random(my_seq, batch_size=2, num_steps=5):\n",
    "    print('X: ', X, '\\nY:', Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45751140",
   "metadata": {},
   "source": [
    "### 顺序分区\n",
    "在迭代过程中，除了对原始序列可以随机抽样外，我们还可以[**保证两个相邻的小批量中的子序列在原始序列上也是相邻的**]。这种策略在基于小批量的迭代过程中保留了拆分的子序列的顺序，因此称为顺序分区。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "8aedbca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_data_iter_sequential(corpus, batch_size, num_steps):  #@save\n",
    "    \"\"\"使用顺序分区生成一个小批量子序列\"\"\"\n",
    "    # 从随机偏移量开始划分序列\n",
    "    offset = random.randint(0, num_steps)\n",
    "    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
    "    Xs = torch.tensor(corpus[offset: offset + num_tokens])\n",
    "    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])\n",
    "    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n",
    "    num_batches = Xs.shape[1] // num_steps\n",
    "    for i in range(0, num_steps * num_batches, num_steps):\n",
    "        X = Xs[:, i: i + num_steps]\n",
    "        Y = Ys[:, i: i + num_steps]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982fccae",
   "metadata": {},
   "source": [
    "基于相同的设置，通过顺序分区[**读取每个小批量的子序列的特征`X`和标签`Y`**]。通过将它们打印出来可以发现：迭代期间来自两个相邻的小批量中的子序列在原始序列中确实是相邻的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "90e91c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  tensor([[ 2,  3,  4,  5,  6],\n",
      "        [18, 19, 20, 21, 22]]) \n",
      "Y: tensor([[ 3,  4,  5,  6,  7],\n",
      "        [19, 20, 21, 22, 23]])\n",
      "X:  tensor([[ 7,  8,  9, 10, 11],\n",
      "        [23, 24, 25, 26, 27]]) \n",
      "Y: tensor([[ 8,  9, 10, 11, 12],\n",
      "        [24, 25, 26, 27, 28]])\n",
      "X:  tensor([[12, 13, 14, 15, 16],\n",
      "        [28, 29, 30, 31, 32]]) \n",
      "Y: tensor([[13, 14, 15, 16, 17],\n",
      "        [29, 30, 31, 32, 33]])\n"
     ]
    }
   ],
   "source": [
    "my_seq = list(range(35))\n",
    "for X, Y in seq_data_iter_sequential(my_seq, batch_size=2, num_steps=5):\n",
    "    print('X: ', X, '\\nY:', Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d97946",
   "metadata": {},
   "source": [
    "现在，我们[**将上面的两个采样函数包装到一个类中**]，以便稍后可以将其用作数据迭代器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "8de95ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataLoader:\n",
    "    \"\"\"加载序列数据的迭代器\"\"\"\n",
    "    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n",
    "        if use_random_iter:\n",
    "            self.data_iter_fn = seq_data_iter_random\n",
    "        else:\n",
    "            self.data_iter_fn = seq_data_iter_sequential\n",
    "        self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens)\n",
    "        self.batch_size, self.num_steps = batch_size, num_steps\n",
    "    def __iter__(self):\n",
    "        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae95d80",
   "metadata": {},
   "source": [
    "[**最后，我们定义了一个函数`load_data_time_machine`，它同时返回数据迭代器和词表**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "66d969be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_time_machine(batch_size, num_steps,\n",
    "                           use_random_iter=False, max_tokens=10000):\n",
    "    \"\"\"返回时光机器数据集的迭代器和词表\"\"\"\n",
    "    data_iter = SeqDataLoader(\n",
    "        batch_size, num_steps, use_random_iter, max_tokens)\n",
    "    return data_iter, data_iter.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff33228",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "f3f701f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = load_data_time_machine(batch_size, \n",
    "                            num_steps,\n",
    "                            use_random_iter=False, \n",
    "                            max_tokens=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "ad6f34f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "30c69922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 35])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_iter))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "f7c500cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 35])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_iter))[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a36904",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "高级API提供了循环神经网络的实现，我们构造一个具有256个隐藏单元的单隐藏层的循环神经网络层`rnn_layer`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "d0dc8a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens = 256\n",
    "rnn_layer = nn.RNN(len(vocab), num_hiddens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a19afe",
   "metadata": {},
   "source": [
    "我们(**使用张量来初始化隐状态**)，它的形状是（隐藏层数，批量大小，隐藏单元数）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "7665c56a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 256])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = torch.zeros((1, batch_size, num_hiddens))\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eae8c95",
   "metadata": {},
   "source": [
    "通过一个隐状态和一个输入，我们就可以用更新后的隐状态计算输出。需要强调的是，`rnn_layer`的输出（`Y`）不涉及输出层的计算：它是指每个时间步的隐状态，这些隐状态可以用作后续输出层的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "305bd5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(size=(num_steps, batch_size, len(vocab)))\n",
    "Y, state_new = rnn_layer(X, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede4c04d",
   "metadata": {},
   "source": [
    "Y是最后一层每一个时间步每个样本的隐状态集合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "5f7dfab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([35, 32, 256])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47698cf5",
   "metadata": {},
   "source": [
    "state_new是最后一个时间步每一层每个样本的隐状态集合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "68327ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 256])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7054c8d",
   "metadata": {},
   "source": [
    "[**我们为一个完整的循环神经网络模型定义了一个`RNNModel`类**]。注意，`rnn_layer`只包含隐藏的循环层，我们还需要创建一个单独的输出层。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "a18a6ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"循环神经网络模型\"\"\"\n",
    "    def __init__(self, rnn_layer, vocab_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = self.rnn.hidden_size\n",
    "        # 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1\n",
    "        if not self.rnn.bidirectional:\n",
    "            self.num_directions = 1\n",
    "            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
    "        else:\n",
    "            self.num_directions = 2\n",
    "            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        X = F.one_hot(inputs.T.long(), self.vocab_size)\n",
    "        X = X.to(torch.float32)\n",
    "        Y, state = self.rnn(X, state)  # Y的形状为：时间步数，批量大小，隐藏单元数\n",
    "        # 全连接层首先将Y的形状改为（时间步数*批量大小， 隐藏单元数）\n",
    "        output = self.linear(Y.reshape(-1, Y.shape[-1]))\n",
    "        return output, state\n",
    "    \n",
    "    def begin_state(self, device, batch_size=1):\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            # nn.GRU以张量作为隐状态\n",
    "            return  torch.zeros((self.num_directions * self.rnn.num_layers,\n",
    "                                 batch_size, self.num_hiddens),\n",
    "                                device=device)\n",
    "        else:\n",
    "            # nn.LSTM以元组作为隐状态\n",
    "            return (torch.zeros((\n",
    "                self.num_directions * self.rnn.num_layers,\n",
    "                batch_size, self.num_hiddens), device=device),\n",
    "                    torch.zeros((\n",
    "                        self.num_directions * self.rnn.num_layers,\n",
    "                        batch_size, self.num_hiddens), device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "6030d636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (rnn): RNN(28, 256)\n",
       "  (linear): Linear(in_features=256, out_features=28, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = RNNModel(rnn_layer, len(vocab))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5dba6c",
   "metadata": {},
   "source": [
    "## 训练与预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "a0cfb85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trian_rnn(net, train_iter, num_epochs, loss, \n",
    "                                  optimizer, device):\n",
    "    net.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        state = None\n",
    "        loss_total = 0\n",
    "        num_batches = 0\n",
    "        for X , Y in train_iter:\n",
    "            # 第一次迭代初始化\n",
    "            if state is None:\n",
    "                state = net.begin_state(device=device, batch_size=X.shape[0])\n",
    "            else:\n",
    "                state.detach_()  # 除第一次迭代，state可以连用，只需去掉梯度信息\n",
    "                \n",
    "            y = Y.T.reshape(-1)\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat, state = net(X, state)\n",
    "\n",
    "            l = loss(y_hat, y.long())\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            \n",
    "            # 梯度裁剪\n",
    "            \n",
    "            params = [p for p in net.parameters() if p.requires_grad]\n",
    "            norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "            if norm > 1:\n",
    "                for param in params:\n",
    "                    param.grad[:] *= 1 / norm\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            loss_total += l * y.numel()\n",
    "            num_batches += y.numel()\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'perplexity in epoch {epoch}: {math.exp(loss_total / num_batches)}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "ce41ec31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity in epoch 0: 21.809054336601108\n",
      "perplexity in epoch 100: 3.53940942681019\n",
      "perplexity in epoch 200: 1.595776524082216\n",
      "perplexity in epoch 300: 1.3879890933519314\n",
      "perplexity in epoch 400: 1.353919754472193\n",
      "perplexity in epoch 500: 1.2952187097994179\n",
      "perplexity in epoch 600: 1.2969301902518724\n",
      "perplexity in epoch 700: 1.2677194429967487\n",
      "perplexity in epoch 800: 1.3034602467949956\n",
      "perplexity in epoch 900: 1.3408058268501963\n"
     ]
    }
   ],
   "source": [
    "trian_rnn(net, train_iter, 1000, loss, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "85dbf265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(prefix, num_preds, net, vocab, device):\n",
    "    \"\"\"在prefix后面生成新字符\"\"\"\n",
    "    state = net.begin_state(batch_size=1, device=device)\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n",
    "    for y in prefix[1:]:  # 预热期\n",
    "        _, state = net(get_input(), state)\n",
    "        outputs.append(vocab[y])\n",
    "    for _ in range(num_preds):  # 预测num_preds步\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    print(''.join([vocab.idx_to_token[i] for i in outputs])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "0f06beb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time traveller come than upast really this incumertal is the fiou\n"
     ]
    }
   ],
   "source": [
    "predict('time traveller ', 50, net, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0867316b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
