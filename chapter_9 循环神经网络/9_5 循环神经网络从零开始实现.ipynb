{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0876a1fd",
   "metadata": {},
   "source": [
    "# 循环神经网络从零开始实现\n",
    "本节从零开始基于循环神经网络实现字符级语言模型。\n",
    "## 读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8fe3fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from d2l import torch as d2l\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05181f48",
   "metadata": {},
   "source": [
    "导入time machine数据集构建词表和数据集迭代器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2595523",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2e909c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 35])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_iter))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0477e3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 35])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_iter))[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b71afc",
   "metadata": {},
   "source": [
    "先回顾一下独热编码，简言之，将每个索引映射为相互不同的单位向量：假设词表中不同词元的数目为$N$（即`len(vocab)`），词元索引的范围为$0$到$N-1$。如果词元的索引是整数$i$，那么我们将创建一个长度为$N$的全$0$向量，并将第$i$处的元素设置为$1$。此向量是原始词元的一个独热向量。索引为$0$和$2$的独热向量如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2edc7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor([0, 2]), len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668846ab",
   "metadata": {},
   "source": [
    "我们每次采样的(**小批量数据形状是二维张量：（批量大小，时间步数）。**)`one_hot`函数将这样一个小批量数据转换成三维张量，张量的最后一个维度等于词表大小（`len(vocab)`）。我们经常转换输入的维度，以便获得形状为**（时间步数，批量大小，词表大小）**的输出。这将使我们能够更方便地通过最外层的维度，一步一步地更新小批量数据的隐状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "843fc836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4],\n",
       "        [5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(10).reshape((2, 5))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3ed0cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 5],\n",
       "        [1, 6],\n",
       "        [2, 7],\n",
       "        [3, 8],\n",
       "        [4, 9]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6890feeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(X.T, len(vocab)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5244117",
   "metadata": {},
   "source": [
    "## 初始化模型参数\n",
    "接下来我们初始化循环神经网络的模型参数，隐藏单元数`num_hidden`是一个可调的超参数。当训练语言模型时，输入输出来自相同的词表，因此，它们具有相同的维度，即词表的大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1e67cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(vocab_size, num_hidden, device):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "    \n",
    "    # 隐藏层参数\n",
    "    W_xh = torch.normal(0, 0.01, (num_inputs, num_hiddens), device=device)\n",
    "    W_hh = torch.normal(0, 0.01, (num_hiddens, num_hiddens), device=device)\n",
    "    b_h = torch.zeros(num_hiddens, device=device)\n",
    "    \n",
    "    # 输出层参数\n",
    "    W_hq = torch.normal(0, 0.01, (num_hiddens, num_outputs), device=device)\n",
    "    b_q = torch.zeros(num_outputs, device=device)\n",
    "    # 附加梯度\n",
    "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1a8a7a",
   "metadata": {},
   "source": [
    "## 循环神经网络模型\n",
    "为了定义循环神经网络模型，我们首先需要一个`init_rnn_state`函数在初始化时返回隐状态。这个函数的返回是一个全为零的张量，形状是（批量大小，隐藏单元数）。在后面的章节中我们将会遇到隐状态包含多个变量的情况， 而使用元组可以更容易地处理些。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31eaa6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_rnn_state(batch_size, num_hiddens, device):\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device),)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ce24c0",
   "metadata": {},
   "source": [
    "下面的`rnn`函数在一个时间步内计算隐状态和输出。循环神经网络模型通过`inputs`最外层的维度实现循环，以便逐时间步更新小批量数据的隐状态`H`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44c0fa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(inputs, state, params):\n",
    "    # inputs的形状：(时间步数量，批量大小，词表大小)\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        H = torch.tanh(torch.matmul(X, W_xh) \n",
    "                       + torch.matmul(H, W_hh)\n",
    "                       + b_h)\n",
    "        Y = torch.matmul(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    # 输出的维度：(批量大小*steps, vocab_size)\n",
    "    return torch.cat(outputs, dim=0), (H,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d17170",
   "metadata": {},
   "source": [
    "定义了所有需要的函数之后，接下来我们创建一个类来包装这些函数，并存储从零开始实现的循环神经网络模型的参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f87e669",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModelScratch:\n",
    "    \"\"\"从零开始实现的循环神经网络模型\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, device,\n",
    "                    get_params, init_state, forward_fn):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.params = get_params(vocab_size, num_hiddens, device)\n",
    "        self.init_state = init_state\n",
    "        self.forward_fn = forward_fn\n",
    "        \n",
    "    def __call__(self, X, state):\n",
    "        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
    "        return self.forward_fn(X, state, self.params)\n",
    "\n",
    "    def begin_state(self, batch_size, device):\n",
    "        return self.init_state(batch_size, self.num_hiddens, device)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394ca98a",
   "metadata": {},
   "source": [
    "让我们检查输出是否具有正确的形状。 例如，隐状态的维数是否保持不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6100bdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c54af55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01a7bdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = RNNModelScratch(len(vocab), num_hiddens, device, get_params,\n",
    "                      init_rnn_state, rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94e30297",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = net.begin_state(X.shape[0],device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0147a503",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y, new_state = net(X.to(device), state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981d37e0",
   "metadata": {},
   "source": [
    "输出形状是（（时间步数$\\times$批量大小，词表大小）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f9791b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 28])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93f30e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f10f33",
   "metadata": {},
   "source": [
    "隐状态形状保持不变，即（批量大小，隐藏单元数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a832180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4bb3ab",
   "metadata": {},
   "source": [
    "## 预测\n",
    "首先定义预测函数来生成`prefix`之后的新字符，其中的`prefix`是一个用户提供的包含多个字符的字符串。在循环遍历`prefix`中的开始字符时，我们不断地将隐状态传递到下一个时间步，但是不生成任何输出。这被称为**预热期**,因为在此期间模型会自我更新隐状态，但不会进行预测。预热器结束后，隐状态的值通常比刚开始的初始值更适合预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69192a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(prefix, num_preds, net, vocab, device):\n",
    "    \"\"\"在prefix后面生成新的字符\"\"\"\n",
    "    state = net.begin_state(batch_size=1, device=device)\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n",
    "    # 预热期\n",
    "    for y in prefix[1:]:\n",
    "        _, state = net(get_input(), state)\n",
    "        outputs.append(vocab[y])\n",
    "    \n",
    "    for _ in range(num_preds):\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    \n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1facbb7d",
   "metadata": {},
   "source": [
    "现在我们可以测试`predict_ch8`函数。我们将前缀指定为`time traveller `，并基于这个前缀生成10个后续字符。鉴于我们还没有训练网络，它会生成荒谬的预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "faae1b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time traveller bbaccccccc'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('time traveller ', 10, net, vocab, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7f75f0",
   "metadata": {},
   "source": [
    "## 梯度剪裁\n",
    "对于长度为$T$的序列，我们在迭代中计算这$T$个时间步上的梯度，将会在反向传播过程中产生长度为$\\mathcal{O}(T)$的矩阵乘法链。当$T$较大时，它可能导致数值不稳定，例如可能导致梯度爆炸或梯度消失。因此，循环神经网络模型往往需要额外的方式来支持稳定训练。\n",
    "\n",
    "对于梯度爆炸而言，一个流行的替代方案是通过将梯度$\\mathbf{g}$投影回给定半径（例如$\\theta$）的球来裁剪梯度$\\mathbf{g}$。\n",
    "如下式：\n",
    "\n",
    "(**$$\\mathbf{g} \\leftarrow \\min\\left(1, \\frac{\\theta}{\\|\\mathbf{g}\\|}\\right) \\mathbf{g}.$$**)\n",
    "\n",
    "通过这样做，我们知道梯度范数永远不会超过$\\theta$，并且更新后的梯度完全与$\\mathbf{g}$的原始方向对齐。\n",
    "\n",
    "\n",
    "下面我们定义一个函数来裁剪模型的梯度，模型是从零开始实现的模型或由高级API构建的模型。我们在此计算了所有模型参数的梯度的范数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "587fbd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(net, theta):\n",
    "    \"\"\"梯度裁剪\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    \n",
    "    else:\n",
    "        params = net.params\n",
    "    \n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] * theta / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e945596c",
   "metadata": {},
   "source": [
    "## 训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a508ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net, train_iter, loss, updater, device, use_random_iter):\n",
    "    \"\"\"训练网络一个迭代周期\"\"\"\n",
    "    state = None\n",
    "    num_samples = 0\n",
    "    loss_total = 0\n",
    "    for X, y in train_iter:\n",
    "        if state is None or use_random_iter:\n",
    "            # 在第一次迭代或者使用随机抽样时初始化state\n",
    "            state = net.begin_state(batch_size=X.shape[0], device=device)\n",
    "        else:\n",
    "            if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
    "                # state对于GRU是一个张量\n",
    "                state.detach_()\n",
    "            else:\n",
    "                # state 对于nn.LSTM或对于我们从零开始实现的模型是个张量\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "        y = y.T.reshape(-1)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_hat, state = net(X, state)\n",
    "        l = loss(y_hat, y.long()).mean()\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            updater.zero_grad()\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            updater.step()\n",
    "        else:\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            # 因为已经调用了mean函数\n",
    "            updater(batch_size=1)\n",
    "        loss_total += l * y.numel()\n",
    "        num_samples += y.numel()\n",
    "    return math.exp(loss_total / num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "08559cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, vocab, lr, num_epochs, device,\n",
    "              use_random_iter=False):\n",
    "    \"\"\"训练模型（定义见第8章）\"\"\"\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    if isinstance(net, nn.Module):\n",
    "        updater = torch.optim.SGD(net.parameters(), lr)\n",
    "    else:\n",
    "        updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)\n",
    "    prediction = lambda prefix: predict(prefix, 50, net, vocab, device)\n",
    "    # 训练和预测\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl= train_epoch(\n",
    "            net, train_iter, loss, updater, device, use_random_iter)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(prediction('time traveller')) \n",
    "            print(f'困惑度 {ppl:.1f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c88c9275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time travellereetreetreetreetreetreetreetreetreetreetreetreetree\n",
      "困惑度 1177917647421301125722144768.0\n",
      "time travellereetteetteetteetteetteetteetteetteetteetteetteettee\n",
      "困惑度 1107438023225415697180917760.0\n",
      "time travellerwe bwe bwe bwe bwe bwe bwe bwe bwe bwe bwe bwe bwe\n",
      "困惑度 603946194427383881641492480.0\n",
      "time travellerelnselnselnselnselnselnselnselnselnselnselnselnsel\n",
      "困惑度 18146961155863814898450432.0\n",
      "time travellera sta sta sta sta sta sta sta sta sta sta sta sta \n",
      "困惑度 3313838390905698890687709184.0\n",
      "time traveller ahn ahn ahn ahn ahn ahn ahn ahn ahn ahn ahn ahn a\n",
      "困惑度 5920979534514001221934645248.0\n",
      "time travellerae  ae  ae  ae  ae  ae  ae  ae  ae  ae  ae  ae  ae\n",
      "困惑度 26475221664868023865664602112.0\n",
      "time travelleroergoergoergoergoergoergoergoergoergoergoergoergoe\n",
      "困惑度 683979650255906701712556032.0\n",
      "time traveller cgo cgo cgo cgo cgo cgo cgo cgo cgo cgo cgo cgo c\n",
      "困惑度 18479722442914788015943450624.0\n",
      "time travellerrsdlrsdlrsdlrsdlrsdlrsdlrsdlrsdlrsdlrsdlrsdlrsdlrs\n",
      "困惑度 25917981278372147945799680.0\n",
      "time travellerueiaueiaueiaueiaueiaueiaueiaueiaueiaueiaueiaueiaue\n",
      "困惑度 5700822765112023566914682880.0\n",
      "time traveller ebo ebo ebo ebo ebo ebo ebo ebo ebo ebo ebo ebo e\n",
      "困惑度 1216477609848050373145657344.0\n",
      "time travellerawemawemawemawemawemawemawemawemawemawemawemawemaw\n",
      "困惑度 157738808792871922921635840.0\n",
      "time traveller inh inh inh inh inh inh inh inh inh inh inh inh i\n",
      "困惑度 210565561897847467532091392.0\n",
      "time travellerieouieouieouieouieouieouieouieouieouieouieouieouie\n",
      "困惑度 6506323242986154616961368064.0\n",
      "time travellernm tnm tnm tnm tnm tnm tnm tnm tnm tnm tnm tnm tnm\n",
      "困惑度 5822582733977949133000933376.0\n",
      "time travellere ite ite ite ite ite ite ite ite ite ite ite ite \n",
      "困惑度 1489970637997587267116335104.0\n",
      "time travellerbcn bcn bcn bcn bcn bcn bcn bcn bcn bcn bcn bcn bc\n",
      "困惑度 10631541871132787474289393664.0\n",
      "time travellerixesixesixesixesixesixesixesixesixesixesixesixesix\n",
      "困惑度 647650560435465878033661952.0\n",
      "time travellerne  ne  ne  ne  ne  ne  ne  ne  ne  ne  ne  ne  ne\n",
      "困惑度 14263472216779186658433040384.0\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 200, 1\n",
    "train(net, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b96af4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9763231e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a45654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
