{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0166842f",
   "metadata": {},
   "source": [
    "# 双向循环神经网络\n",
    "在序列学习中，我们以往假设的目标是：在给定观测的情况下，对下一个输出进行建模。虽然这是一个典型情景，但不是唯一的。还可能发生什么其它的情况呢？我们考虑以下三个在文本序列中填空的任务：\n",
    "\n",
    "* 我`___`。\n",
    "* 我`___`饿了。\n",
    "* 我`___`饿了，我可以吃半头猪。\n",
    "\n",
    "根据可获得的信息量，我们可以用不同的词填空，如“很高兴”（\"happy\"）、“不”（\"not\"）和“非常”（\"very\"）。很明显，每个短语的“下文”传达了重要信息（如果有的话），而这些信息关乎到选择哪个词来填空，所以无法利用这一点的序列模型将在相关任务上表现不佳。为了解决这个问题，我们可以使用双向循环神经网络。\n",
    "\n",
    "## 双向模型\n",
    "双向模型只需要增加一个“从最后一个词元开始从后向前运行”的循环神经网络，而不是只有一个在前向模式下“从第一个词元开始运行”的循环神经网络。*双向循环神经网络*（bidirectional RNNs）\n",
    "添加了反向传递信息的隐藏层，以便更灵活地处理此类信息。下图描述了具有单个隐藏层的双向循环神经网络的架构。\n",
    "\n",
    "![双向循环神经网络架构](./birnn.svg)\n",
    "\n",
    ":label:`fig_birnn`\n",
    "\n",
    "双向循环神经网络是由 :cite:`Schuster.Paliwal.1997`提出的，让我们看看这样一个网络的细节。对于任意时间步$t$，给定一个小批量的输入数据$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$（样本数：$n$，每个示例中的输入数：$d$），并且令隐藏层激活函数为$\\phi$。在双向架构中，我们设该时间步的前向和反向隐状态分别为\n",
    "$\\overrightarrow{\\mathbf{H}}_t  \\in \\mathbb{R}^{n \\times h}$和$\\overleftarrow{\\mathbf{H}}_t  \\in \\mathbb{R}^{n \\times h}$，其中$h$是隐藏单元的数目。前向和反向隐状态的更新如下：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\overrightarrow{\\mathbf{H}}_t &= \\phi(\\mathbf{X}_t \\mathbf{W}_{xh}^{(f)} + \\overrightarrow{\\mathbf{H}}_{t-1} \\mathbf{W}_{hh}^{(f)}  + \\mathbf{b}_h^{(f)}),\\\\\n",
    "\\overleftarrow{\\mathbf{H}}_t &= \\phi(\\mathbf{X}_t \\mathbf{W}_{xh}^{(b)} + \\overleftarrow{\\mathbf{H}}_{t+1} \\mathbf{W}_{hh}^{(b)}  + \\mathbf{b}_h^{(b)}),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中，权重$\\mathbf{W}_{xh}^{(f)} \\in \\mathbb{R}^{d \\times h}, \\mathbf{W}_{hh}^{(f)} \\in \\mathbb{R}^{h \\times h}, \\mathbf{W}_{xh}^{(b)} \\in \\mathbb{R}^{d \\times h},\\mathbf{W}_{hh}^{(b)} \\in \\mathbb{R}^{h \\times h}$和偏置$\\mathbf{b}_h^{(f)} \\in \\mathbb{R}^{1 \\times h},\\mathbf{b}_h^{(b)} \\in \\mathbb{R}^{1 \\times h}$都是模型参数。\n",
    "\n",
    "接下来，将前向隐状态$\\overrightarrow{\\mathbf{H}}_t$和反向隐状态$\\overleftarrow{\\mathbf{H}}_t$连接起来，获得需要送入输出层的隐状态$\\mathbf{H}_t \\in \\mathbb{R}^{n \\times 2h}$。在具有多个隐藏层的深度双向循环神经网络中，该信息作为输入传递到下一个双向层。最后，输出层计算得到的输出为$\\mathbf{O}_t \\in \\mathbb{R}^{n \\times q}$（$q$是输出单元的数目）：\n",
    "\n",
    "$$\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{hq} + \\mathbf{b}_q.$$\n",
    "\n",
    "这里，权重矩阵$\\mathbf{W}_{hq} \\in \\mathbb{R}^{2h \\times q}$\n",
    "和偏置$\\mathbf{b}_q \\in \\mathbb{R}^{1 \\times q}$是输出层的模型参数。事实上，这两个方向可以拥有不同数量的隐藏单元。\n",
    "\n",
    "## 局限性和应用范围\n",
    "双向循环神经网络的一个关键特性是：使用来自序列两端的信息来估计输出。也就是说，我们使用来自过去和未来的观测信息来预测当前的观测。但是在对下一个词元进行预测的情况中，这样的模型并不是我们所需的。因为在预测下一个词元时，我们终究无法知道下一个词元的下文是什么。\n",
    "\n",
    "另一个严重问题是，双向循环神经网络的计算速度非常慢。其主要原因是网络的前向传播需要在双向层中进行前向和后向递归，并且网络的反向传播还依赖于前向传播的结果。因此，梯度求解将有一个非常长的链。\n",
    "\n",
    "双向循环神经网络通常用于对序列抽取特征、填充缺失的单词、词元注释（例如，用于命名实体识别）以及作为序列处理流水线中的一个步骤对序列进行编码（例如，用于机器翻译）等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88fef1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
