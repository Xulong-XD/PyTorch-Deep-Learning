{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f2f24df",
   "metadata": {},
   "source": [
    "# Seq2Seq与机器翻译\n",
    "机器翻译中的输入序列和输出序列都是长度可变的,为了解决这类问题，我们设计了一个通用的”编码器－解码器“架构。本节，我们将使用两个循环神经网络的编码器和解码器，并将其应用于*序列到序列*（sequence to sequence，seq2seq）类的学习任务.\n",
    "\n",
    "遵循编码器－解码器架构的设计原则，循环神经网络编码器使用长度可变的序列作为输入，将其转换为固定形状的隐状态。换言之，输入序列的信息被*编码*到循环神经网络编码器的隐状态中。为了连续生成输出序列的词元，独立的循环神经网络解码器是基于输入序列的编码信息和输出序列已经看见的或者生成的词元来预测下一个词元。下图演示了如何在机器翻译中使用两个循环神经网络进行序列到序列学习。\n",
    "\n",
    "![使用循环神经网络编码器和循环神经网络解码器的序列到序列学习](./seq2seq.svg)\n",
    "\n",
    ":label:`fig_seq2seq`\n",
    "\n",
    "在 :numref:`fig_seq2seq`中，特定的“&lt;eos&gt;”表示序列结束词元。\n",
    "一旦输出序列生成此词元，模型就会停止预测。在循环神经网络解码器的初始化时间步，有两个特定的设计决定：首先，特定“&lt;bos&gt;”表示序列开始词元，它是解码器的输入序列的第一个词元。其次，使用循环神经网络编码器最终的隐状态来初始化解码器的隐状态。\n",
    "\n",
    "下面，我们动手构建 :numref:`fig_seq2seq`的设计，并将基于 :numref:`sec_machine_translation`中介绍的“英－法”数据集来训练这个机器翻译模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2808e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd00a63",
   "metadata": {},
   "source": [
    "## 编码器\n",
    "\n",
    "从技术上讲，编码器将长度可变的输入序列转换成形状固定的上下文变量$\\mathbf{c}$，并且将输入序列的信息在该上下文变量中进行编码。\n",
    "如 :numref:`fig_seq2seq`所示，可以使用循环神经网络来设计编码器。\n",
    "\n",
    "考虑由一个序列组成的样本（批量大小是$1$）。假设输入序列是$x_1, \\ldots, x_T$，其中$x_t$是输入文本序列中的第$t$个词元。在时间步$t$，循环神经网络将词元$x_t$的输入特征向量$\\mathbf{x}_t$和$\\mathbf{h} _{t-1}$（即上一时间步的隐状态）转换为$\\mathbf{h}_t$（即当前步的隐状态）。使用一个函数$f$来描述循环神经网络的循环层所做的变换：\n",
    "\n",
    "$$\\mathbf{h}_t = f(\\mathbf{x}_t, \\mathbf{h}_{t-1}). $$\n",
    "\n",
    "总之，编码器通过选定的函数$q$，将所有时间步的隐状态转换为上下文变量：\n",
    "\n",
    "$$\\mathbf{c} =  q(\\mathbf{h}_1, \\ldots, \\mathbf{h}_T).$$\n",
    "\n",
    "比如，当选择$q(\\mathbf{h}_1, \\ldots, \\mathbf{h}_T) = \\mathbf{h}_T$时（就像 :numref:`fig_seq2seq`中一样），上下文变量仅仅是输入序列在最后时间步的隐状态$\\mathbf{h}_T$。\n",
    "\n",
    "到目前为止，我们使用的是一个单向循环神经网络来设计编码器，其中隐状态只依赖于输入子序列，这个子序列是由输入序列的开始位置到隐状态所在的时间步的位置（包括隐状态所在的时间步）组成。我们也可以使用双向循环神经网络构造编码器，其中隐状态依赖于两个输入子序列，两个子序列是由隐状态所在的时间步的位置之前的序列和之后的序列（包括隐状态所在的时间步），因此隐状态对整个序列的信息都进行了编码。\n",
    "\n",
    "现在，让我们[**实现循环神经网络编码器**]。注意，我们使用了*嵌入层*（embedding layer）来获得输入序列中每个词元的特征向量。\n",
    "嵌入层的权重是一个矩阵，其行数等于输入词表的大小（`vocab_size`），其列数等于特征向量的维度（`embed_size`）。\n",
    "对于任意输入词元的索引$i$，嵌入层获取权重矩阵的第$i$行（从$0$开始）以返回其特征向量。另外，本文选择了一个多层门控循环单元来实现编码器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d03b1613",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqEncoder(nn.Module):\n",
    "    \"\"\"用于序列到序列学习的循环神经网络编码器\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                    dropout=0):\n",
    "        super(Seq2SeqEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers,\n",
    "                             dropout=dropout, bidirectional=False)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # 输出'X'的形状：(batch_size,num_steps,embed_size)\n",
    "        X = self.embedding(X)  \n",
    "        # 在循环神经网络模型中，第一个轴对应于时间步\n",
    "        X = X.permute(1, 0, 2)  \n",
    "        # 如果未提及状态，则默认为0\n",
    "        output, state = self.rnn(X)\n",
    "        # output的形状:(num_steps,batch_size,num_hiddens)\n",
    "        # state的形状:(num_layers,batch_size,num_hiddens)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140a6490",
   "metadata": {},
   "source": [
    "下面，我们实例化[**上述编码器的实现**]：我们使用一个两层门控循环单元编码器，其隐藏单元数为$16$。给定一小批量的输入序列`X`（批量大小为$4$，时间步为$7$）。在完成所有时间步后，最后一层的隐状态的输出是一个张量（`output`由编码器的循环层返回），其形状为（时间步数，批量大小，隐藏单元数）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "322625aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16,\n",
    "                         num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d31cc9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.zeros((4, 7), dtype=torch.long)\n",
    "output, state = encoder(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e6d8e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 4, 16])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "061a9037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 16])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30366b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 4, 16])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[-1].repeat(7, 1 , 1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ce3f9d",
   "metadata": {},
   "source": [
    "## 解码器\n",
    "正如上文提到的，编码器输出的上下文变量$\\mathbf{c}$对整个输入序列$x_1, \\ldots, x_T$进行编码。来自训练数据集的输出序列$y_1, y_2, \\ldots, y_{T'}$，对于每个时间步$t'$（与输入序列或编码器的时间步$t$不同），解码器输出$y_{t'}$的概率取决于先前的输出子序列\n",
    "$y_1, \\ldots, y_{t'-1}$和上下文变量$\\mathbf{c}$，即$P(y_{t'} \\mid y_1, \\ldots, y_{t'-1}, \\mathbf{c})$。\n",
    "\n",
    "为了在序列上模型化这种条件概率，我们可以使用另一个循环神经网络作为解码器。在输出序列上的任意时间步$t^\\prime$，循环神经网络将来自上一时间步的输出$y_{t^\\prime-1}$和上下文变量$\\mathbf{c}$作为其输入，然后在当前时间步将它们和上一隐状态$\\mathbf{s}_{t^\\prime-1}$转换为隐状态$\\mathbf{s}_{t^\\prime}$。\n",
    "因此，可以使用函数$g$来表示解码器的隐藏层的变换：\n",
    "\n",
    "$$\\mathbf{s}_{t^\\prime} = g(y_{t^\\prime-1}, \\mathbf{c}, \\mathbf{s}_{t^\\prime-1}).$$\n",
    ":eqlabel:`eq_seq2seq_s_t`\n",
    "\n",
    "在获得解码器的隐状态之后，我们可以使用输出层和softmax操作\n",
    "来计算在时间步$t^\\prime$时输出$y_{t^\\prime}$的条件概率分布\n",
    "$P(y_{t^\\prime} \\mid y_1, \\ldots, y_{t^\\prime-1}, \\mathbf{c})$。\n",
    "\n",
    "根据 :numref:`fig_seq2seq`，当实现解码器时，我们直接使用编码器最后一个时间步的隐状态来初始化解码器的隐状态。这就要求使用循环神经网络实现的编码器和解码器具有相同数量的层和隐藏单元。为了进一步包含经过编码的输入序列的信息，上下文变量在所有的时间步与解码器的输入进行拼接（concatenate）。为了预测输出词元的概率分布，\n",
    "在循环神经网络解码器的最后一层使用全连接层来变换隐状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbcb3073",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(nn.Module):\n",
    "    \"\"\"用于序列到序列学习的循环神经网络解码器\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size,num_hiddens, num_layers,dropout=0):\n",
    "        super(Seq2SeqDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers, \n",
    "                         dropout=dropout, bidirectional=False)\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "    \n",
    "    \n",
    "    # 用编码器最后的state初始化解码器的状态    \n",
    "    def init_state(self, enc_outputs):\n",
    "        return enc_outputs[1]\n",
    "    \n",
    "    def forward(self, X, state):\n",
    "        # X'的形状：(num_steps, batch_size, embed_size)\n",
    "        X = self.embedding(X).permute(1, 0, 2)  \n",
    "        # 广播context，使其具有与X相同的num_steps\n",
    "        context = state[-1].repeat(X.shape[0], 1, 1)\n",
    "        X_and_context = torch.cat((X, context), 2)\n",
    "        output, state = self.rnn(X_and_context, state)\n",
    "        output = self.dense(output).permute(1, 0, 2)\n",
    "        # output的形状:(batch_size,num_steps,vocab_size)\n",
    "        # state的形状:(num_layers,batch_size,num_hiddens)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805b9df1",
   "metadata": {},
   "source": [
    "下面，我们用与前面提到的编码器中相同的超参数来[**实例化解码器**]。如我们所见，解码器的输出形状变为（批量大小，时间步数，词表大小），其中张量的最后一个维度存储预测的词元分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8383be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16,\n",
    "                         num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf825843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqDecoder(\n",
       "  (embedding): Embedding(10, 8)\n",
       "  (rnn): GRU(24, 16, num_layers=2)\n",
       "  (dense): Linear(in_features=16, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33e25469",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = decoder.init_state(encoder(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "112c9aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 16])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e27f0060",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.zeros((4, 7), dtype=torch.long)\n",
    "output, state = decoder(X, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd546fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 7, 10]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape, state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b4e74",
   "metadata": {},
   "source": [
    "总之，上述循环神经网络“编码器－解码器”模型中的各层如\n",
    " :numref:`fig_seq2seq_details`所示。\n",
    "\n",
    "![循环神经网络编码器-解码器模型中的层](./seq2seq-details.svg)\n",
    "\n",
    ":label:`fig_seq2seq_details`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ca50e5",
   "metadata": {},
   "source": [
    "接下来，我们将编码器和解码器整合在一起："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9e3d7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, enc_X, dec_X):\n",
    "        enc_outputs = self.encoder(enc_X)\n",
    "        dec_state = self.decoder.init_state(enc_outputs)\n",
    "        return self.decoder(dec_X, dec_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f496a57",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "\n",
    "在每个时间步，解码器预测了输出词元的概率分布。类似于语言模型，可以使用softmax来获得分布，并通过计算交叉熵损失函数来进行优化。\n",
    "回想一下 :numref:`sec_machine_translation`中，特定的填充词元被添加到序列的末尾，因此不同长度的序列可以以相同形状的小批量加载。但是，我们应该将填充词元的预测排除在损失函数的计算之外。\n",
    "\n",
    "为此，我们可以使用下面的`sequence_mask`函数[**通过零值化屏蔽不相关的项**]，以便后面任何不相关预测的计算都是与零的乘积，结果都等于零。例如，如果两个序列的有效长度（不包括填充词元）分别为$1$和$2$，则第一个序列的第一项和第二个序列的前两项之后的剩余项将被清除为零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "794b1274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(X, valid_len, value=0):\n",
    "    \"\"\"在序列中屏蔽不相关的项\"\"\"\n",
    "    maxlen = X.size(1)\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32,\n",
    "                        device=X.device)[None, :] < valid_len[:, None]\n",
    "    X[~mask] = value\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8c52dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [4, 5, 0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "valid_len = torch.tensor([1, 2])\n",
    "sequence_mask(X, valid_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152b7d23",
   "metadata": {},
   "source": [
    "现在我们可以通过扩展softmax交叉熵损失函数来遮蔽不相关的预测。最初，所有预测词元的掩码都设置为1，一旦给定了有效长度，与填充词元对应的掩码将被设置为0。最后，将所有词元的损失乘以掩码，以过滤掉损失中填充次元产生的不相关的预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5b3076c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    \"\"\"带遮蔽的softmax交叉熵损失函数\"\"\"\n",
    "    # pred的形状：(batch_size,num_steps,vocab_size)\n",
    "    # label的形状：(batch_size,num_steps)\n",
    "    # valid_len的形状：(batch_size,)\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        weights = torch.ones_like(label)\n",
    "        weights = sequence_mask(weights, valid_len)\n",
    "        self.reduction = 'none'\n",
    "        unweighted_loss = super().forward(pred.permute(0, 2, 1), label)\n",
    "        weighted_loss = (unweighted_loss * weights).mean(dim=1)\n",
    "        return weighted_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df68260",
   "metadata": {},
   "source": [
    "我们可以创建三个相同的序列来进行[**代码健全性检查**]，然后分别指定这些序列的有效长度为$4$、$2$和$0$。结果就是，第一个序列的损失应为第二个序列的两倍，而第三个序列的损失应为零。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fa42c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3026, 1.1513, 0.0000])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = MaskedSoftmaxCELoss()\n",
    "loss(torch.ones(3, 4, 10), torch.ones((3, 4), dtype=torch.long),\n",
    "     torch.tensor([4, 2, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5535925f",
   "metadata": {},
   "source": [
    "## 训练\n",
    "在下面的循环训练过程中，特定的序列开始词元（“&lt;bos&gt;”）和\n",
    "原始的输出序列（不包括序列结束词元“&lt;eos&gt;”）拼接在一起作为解码器的输入。这被称为*强制教学*（teacher forcing）。而在预测时，只将上一个时间步的*预测*得到的词元作为解码器的当前输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b86e837",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n",
    "batch_size, num_steps = 64, 10\n",
    "lr, num_epochs = 0.005, 300\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e939c2a",
   "metadata": {},
   "source": [
    "加载数据集，得到数据迭代器和源、目标词表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33c0a214",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf22e048",
   "metadata": {},
   "source": [
    "实例化编码器和解码器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "11241f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers,\n",
    "                        dropout)\n",
    "decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers,\n",
    "                        dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9fd910",
   "metadata": {},
   "source": [
    "实例化编码器解码器架构，得到seq2seq网络："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "240720fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = EncoderDecoder(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8110f6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Seq2SeqEncoder(\n",
       "    (embedding): Embedding(184, 32)\n",
       "    (rnn): GRU(32, 32, num_layers=2, dropout=0.1)\n",
       "  )\n",
       "  (decoder): Seq2SeqDecoder(\n",
       "    (embedding): Embedding(201, 32)\n",
       "    (rnn): GRU(64, 32, num_layers=2, dropout=0.1)\n",
       "    (dense): Linear(in_features=32, out_features=201, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "211edb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):\n",
    "    \"\"\"训练序列到序列模型\"\"\"\n",
    "    def xavier_init_weights(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        if type(m) == nn.GRU:\n",
    "            for param in m._flat_weights_names:\n",
    "                if \"weight\" in param:\n",
    "                    nn.init.xavier_uniform_(m._parameters[param])\n",
    "\n",
    "    net.apply(xavier_init_weights)\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss = MaskedSoftmaxCELoss()\n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_total = 0\n",
    "        num_samples = 0\n",
    "        \n",
    "        for batch in data_iter:\n",
    "            optimizer.zero_grad()\n",
    "            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n",
    "            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0],\n",
    "                          device=device).reshape(-1, 1)\n",
    "            dec_input = torch.cat([bos, Y[:, :-1]], 1)\n",
    "            Y_hat, _ = net(X, dec_input)\n",
    "            l = loss(Y_hat, Y, Y_valid_len)  # 每个样本一个损失值\n",
    "            l.sum().backward()  # 损失函数的标量进行“反向传播”\n",
    "            d2l.grad_clipping(net, 1)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_total += l.sum()\n",
    "            num_samples += Y_valid_len.sum()\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f'loss in epoch {epoch + 1}: {loss_total / num_samples:.3f}')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892d0385",
   "metadata": {},
   "source": [
    "现在，在机器翻译数据集上，我们可以[**创建和训练一个循环神经网络“编码器－解码器”模型**]用于序列到序列的学习。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ef568b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in epoch 50: 0.065\n",
      "loss in epoch 100: 0.032\n",
      "loss in epoch 150: 0.024\n",
      "loss in epoch 200: 0.021\n",
      "loss in epoch 250: 0.020\n",
      "loss in epoch 300: 0.020\n"
     ]
    }
   ],
   "source": [
    "train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fffc37",
   "metadata": {},
   "source": [
    "## 预测\n",
    "为了采用一个接着一个词元的方式预测输出序列，每个解码器当前时间步的输入都将来自前一时间步的预测词元。与训练类似，序列开始词元（“&lt;bos&gt;”）在初始时间步被输入到解码器中。当输出序列的预测遇到序列结束词元（“&lt;eos&gt;”）时，预测就结束了。\n",
    "\n",
    "![使用循环神经网络编码器-解码器逐词元地预测输出序列。](./seq2seq-predict.svg)\n",
    "\n",
    ":label:`fig_seq2seq_predict`\n",
    "\n",
    "我们将在 :numref:`sec_beam-search`中介绍不同的序列生成策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0918095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, \n",
    "                        num_steps, device):\n",
    "    \"\"\"序列到序列模型的预测\"\"\"\n",
    "    # 在预测时将net设置为评估模式\n",
    "    net.eval()\n",
    "    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [\n",
    "        src_vocab['<eos>']]\n",
    "    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n",
    "    # 添加批量轴\n",
    "    enc_X = torch.unsqueeze(\n",
    "        torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)\n",
    "    enc_outputs = net.encoder(enc_X)\n",
    "    dec_state = net.decoder.init_state(enc_outputs)\n",
    "    # 添加批量轴\n",
    "    dec_X = torch.unsqueeze(torch.tensor(\n",
    "        [tgt_vocab['<bos>']], dtype=torch.long, device=device), dim=0)\n",
    "    output_seq = []\n",
    "    for _ in range(num_steps):\n",
    "        Y, dec_state = net.decoder(dec_X, dec_state)\n",
    "        # 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入\n",
    "        dec_X = Y.argmax(dim=2)\n",
    "        pred = dec_X.squeeze(dim=0).type(torch.int32).item()\n",
    "        # 一旦序列结束词元被预测，输出序列的生成就完成了\n",
    "        if pred == tgt_vocab['<eos>']:\n",
    "            break\n",
    "        output_seq.append(pred)\n",
    "    return ' '.join(tgt_vocab.to_tokens(output_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85070ca6",
   "metadata": {},
   "source": [
    "利用训练好的循环神经网络“编码器－解码器”模型，[**将几个英语句子翻译成法语**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5ae84dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "854c6b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go . => va !\n",
      "i lost . => j'ai perdu ici .\n",
      "he's calm . => il est partie .\n",
      "i'm home . => je suis bien <unk> mouvement de recul\n"
     ]
    }
   ],
   "source": [
    "for eng in engs:\n",
    "    translation = predict_seq2seq(\n",
    "        net, eng, src_vocab, tgt_vocab, num_steps, device)\n",
    "    print(f'{eng} => {translation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ee9cca",
   "metadata": {},
   "source": [
    "## 评估指标：BLEU分数\n",
    "我们可以通过与真实的标签序列进行比较来评估预测序列。虽然 :cite:`Papineni.Roukos.Ward.ea.2002`提出的BLEU（bilingual evaluation understudy）最先是用于评估机器翻译的结果，但现在它已经被广泛用于测量许多应用的输出序列的质量。原则上说，对于预测序列中的任意$n$元语法（n-grams），BLEU的评估都是这个$n$元语法是否出现在标签序列中。\n",
    "\n",
    "我们将BLEU定义为：\n",
    "\n",
    "$$ \\exp\\left(\\min\\left(0, 1 - \\frac{\\mathrm{len}_{\\text{label}}}{\\mathrm{len}_{\\text{pred}}}\\right)\\right) \\prod_{n=1}^k p_n^{1/2^n},$$\n",
    "\n",
    ":eqlabel:`eq_bleu`\n",
    "\n",
    "其中$\\mathrm{len}_{\\text{label}}$表示标签序列中的词元数和\n",
    "$\\mathrm{len}_{\\text{pred}}$表示预测序列中的词元数，$k$是用于匹配的最长的$n$元语法。另外，用$p_n$表示$n$元语法的精确度，它是两个数量的比值：第一个是预测序列与标签序列中匹配的$n$元语法的数量，第二个是预测序列中$n$元语法的数量的比率。具体地说，给定标签序列$A$、$B$、$C$、$D$、$E$、$F$和预测序列$A$、$B$、$B$、$C$、$D$，我们有$p_1 = 4/5$、$p_2 = 3/4$、$p_3 = 1/3$和$p_4 = 0$。\n",
    "\n",
    "根据 :eqref:`eq_bleu`中BLEU的定义，当预测序列与标签序列完全相同时，BLEU为$1$。此外，由于$n$元语法越长则匹配难度越大，所以BLEU为更长的$n$元语法的精确度分配更大的权重。具体来说，当$p_n$固定时，$p_n^{1/2^n}$会随着$n$的增长而增加（原始论文使用$p_n^{1/n}$）。而且，由于预测的序列越短获得的$p_n$值越高，所以 :eqref:`eq_bleu`中乘法项之前的系数用于惩罚较短的预测序列。例如，当$k=2$时，给定标签序列$A$、$B$、$C$、$D$、$E$、$F$和预测序列$A$、$B$，尽管$p_1 = p_2 = 1$，惩罚因子$\\exp(1-6/2) \\approx 0.14$会降低BLEU。\n",
    "\n",
    "[**BLEU的代码实现**]如下。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a5be2792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu(pred_seq, label_seq, k):  #@save\n",
    "    \"\"\"计算BLEU\"\"\"\n",
    "    pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')\n",
    "    len_pred, len_label = len(pred_tokens), len(label_tokens)\n",
    "    score = math.exp(min(0, 1 - len_label / len_pred))\n",
    "    for n in range(1, k + 1):\n",
    "        num_matches, label_subs = 0, collections.defaultdict(int)\n",
    "        for i in range(len_label - n + 1):\n",
    "            label_subs[' '.join(label_tokens[i: i + n])] += 1\n",
    "        for i in range(len_pred - n + 1):\n",
    "            if label_subs[' '.join(pred_tokens[i: i + n])] > 0:\n",
    "                num_matches += 1\n",
    "                label_subs[' '.join(pred_tokens[i: i + n])] -= 1\n",
    "        score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2b7830f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go . => va !, bleu 1.000\n",
      "i lost . => j'ai perdu ici ., bleu 0.658\n",
      "he's calm . => il est partie ., bleu 0.658\n",
      "i'm home . => je suis bien <unk> mouvement de recul, bleu 0.342\n"
     ]
    }
   ],
   "source": [
    "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
    "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
    "for eng, fra in zip(engs, fras):\n",
    "    translation = predict_seq2seq(\n",
    "        net, eng, src_vocab, tgt_vocab, num_steps, device)\n",
    "    print(f'{eng} => {translation}, bleu {bleu(translation, fra, k=2):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd14cf2",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 根据“编码器-解码器”架构的设计，\n",
    "  我们可以使用两个循环神经网络来设计一个序列到序列学习的模型。\n",
    "* 在实现编码器和解码器时，我们可以使用多层循环神经网络。\n",
    "* 我们可以使用遮蔽来过滤不相关的计算，例如在计算损失时。\n",
    "* 在“编码器－解码器”训练中，强制教学方法将原始输出序列（而非预测结果）输入解码器。\n",
    "* BLEU是一种常用的评估方法，它通过测量预测序列和标签序列之间的$n$元语法的匹配度来评估预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "801cbb84",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 1455] 页面文件太小，无法完成操作。 Error loading \"D:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m queries \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m256\u001b[39m))\n\u001b[0;32m      3\u001b[0m eys \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m256\u001b[39m))\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\__init__.py:129\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    127\u001b[0m     err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(last_error)\n\u001b[0;32m    128\u001b[0m     err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    131\u001b[0m     is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 1455] 页面文件太小，无法完成操作。 Error loading \"D:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "queries = torch.normal(0, 1, (32, 1, 256))\n",
    "eys = torch.normal(0, 1, (32, 10, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a16a6f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
