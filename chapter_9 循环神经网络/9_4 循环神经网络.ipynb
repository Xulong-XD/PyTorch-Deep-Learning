{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e88cc3a",
   "metadata": {},
   "source": [
    "# 循环神经网络\n",
    "在语言模型中，我们介绍了$n-gram$模型，其中单词$x_t$在时间步$t$的条件概率仅取决于前面$n-1$个单词。对于时间步$t-(n-1)$之前的单词，\n",
    "如果我们想将其可能产生的影响合并到$x_t$上，需要增加$n$，然而模型参数的数量也会随之呈指数增长，因为词表$\\mathcal{V}$需要存储$|\\mathcal{V}|^n$个数字，因此与其将$P(x_t \\mid x_{t-1}, \\ldots, x_{t-n+1})$模型化，不如使用隐变量模型：\n",
    "\n",
    "$$P(x_t \\mid x_{t-1}, \\ldots, x_1) \\approx P(x_t \\mid h_{t-1}),$$\n",
    "\n",
    "其中$h_{t-1}$是*隐状态*（hidden state），也称为*隐藏变量*（hidden variable），它存储了到时间步$t-1$的序列信息。通常，我们可以基于当前输入$x_{t}$和先前隐状态$h_{t-1}$来计算时间步$t$处的任何时间的隐状态：\n",
    "\n",
    "$$h_t = f(x_{t}, h_{t-1}).$$\n",
    ":eqlabel:`eq_ht_xt`\n",
    "\n",
    "*循环神经网络*（recurrent neural networks，RNNs）是具有隐状态的神经网络。在介绍循环神经网络模型之前，我们首先回顾:numref:`sec_mlp`中介绍的多层感知机模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ecf7b8",
   "metadata": {},
   "source": [
    "## 无隐状态的神经网络\n",
    "\n",
    "让我们来看一看只有单隐藏层的多层感知机。设隐藏层的激活函数为$\\phi$，给定一个小批量样本$\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$，其中批量大小为$n$，输入维度为$d$，则隐藏层的输出$\\mathbf{H} \\in \\mathbb{R}^{n \\times h}$通过下式计算：\n",
    "\n",
    "$$\\mathbf{H} = \\phi(\\mathbf{X} \\mathbf{W}_{xh} + \\mathbf{b}_h).$$\n",
    "\n",
    "我们拥有的隐藏层权重参数为$\\mathbf{W}_{xh} \\in \\mathbb{R}^{d \\times h}$，偏置参数为$\\mathbf{b}_h \\in \\mathbb{R}^{1 \\times h}$，以及隐藏单元的数目为$h$。因此求和时可以应用广播机制（见 :numref:`subsec_broadcasting`）。接下来，将隐藏变量$\\mathbf{H}$用作输出层的输入。输出层由下式给出：\n",
    "\n",
    "$$\\mathbf{O} = \\mathbf{H} \\mathbf{W}_{hq} + \\mathbf{b}_q,$$\n",
    "\n",
    "其中，$\\mathbf{O} \\in \\mathbb{R}^{n \\times q}$是输出变量，\n",
    "$\\mathbf{W}_{hq} \\in \\mathbb{R}^{h \\times q}$是权重参数，\n",
    "$\\mathbf{b}_q \\in \\mathbb{R}^{1 \\times q}$是输出层的偏置参数。\n",
    "如果是分类问题，我们可以用$\\text{softmax}(\\mathbf{O})$\n",
    "来计算输出类别的概率分布。\n",
    "\n",
    "只要可以随机选择“特征-标签”对，\n",
    "并且通过自动微分和随机梯度下降能够学习网络参数就可以了。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f615c3",
   "metadata": {},
   "source": [
    "## 有隐状态的循环神经网络\n",
    "有了隐状态后，情况就完全不同了。假设我们在时间步$t$有小批量输入$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$。换言之，对于$n$个序列样本的小批量，$\\mathbf{X}_t$的每一行对应于来自该序列的时间步$t$处的一个样本。接下来，用$\\mathbf{H}_t  \\in \\mathbb{R}^{n \\times h}$表示时间步$t$的隐藏变量。与多层感知机不同的是，我们在这里保存了前一个时间步的隐藏变量$\\mathbf{H}_{t-1}$，并引入了一个新的权重参数$\\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$，来描述如何在当前时间步中使用前一个时间步的隐藏变量。具体地说，当前时间步隐藏变量由当前时间步的输入与前一个时间步的隐藏变量一起计算得出：\n",
    "\n",
    "$$\\mathbf{H}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh}  + \\mathbf{b}_h).$$\n",
    "\n",
    "从相邻时间步的隐藏变量$\\mathbf{H}_t$和$\\mathbf{H}_{t-1}$之间的关系可知，这些变量捕获并保留了序列直到其当前时间步的历史信息，就如当前时间步下神经网络的状态或记忆，因此这样的隐藏变量被称为*隐状态*（hidden state）。由于在当前时间步中，隐状态使用的定义与前一个时间步中使用的定义相同，因此$\\mathbf{H}_t$的计算是*循环的*（recurrent）。于是基于循环计算的隐状态神经网络被命名为*循环神经网络*（recurrent neural network）。在循环神经网络中执行 $\\mathbf{H}_t$计算的层称为*循环层*（recurrent layer）。\n",
    "\n",
    "有许多不同的方法可以构建循环神经网络，由上面$\\mathbf{H}_t$定义的隐状态的循环神经网络是非常常见的一种。对于时间步$t$，输出层的输出类似于多层感知机中的计算：\n",
    "\n",
    "$$\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{hq} + \\mathbf{b}_q.$$\n",
    "\n",
    "循环神经网络的参数包括隐藏层的权重$\\mathbf{W}_{xh} \\in \\mathbb{R}^{d \\times h}, \\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$和偏置$\\mathbf{b}_h \\in \\mathbb{R}^{1 \\times h}$，以及输出层的权重$\\mathbf{W}_{hq} \\in \\mathbb{R}^{h \\times q}$\n",
    "和偏置$\\mathbf{b}_q \\in \\mathbb{R}^{1 \\times q}$。值得一提的是，即使在不同的时间步，循环神经网络也总是使用这些模型参数。因此，循环神经网络的参数开销不会随着时间步的增加而增加。\n",
    "\n",
    " :numref:`fig_rnn`展示了循环神经网络在三个相邻时间步的计算逻辑。\n",
    "在任意时间步$t$，隐状态的计算可以被视为：\n",
    "\n",
    "1. 拼接当前时间步$t$的输入$\\mathbf{X}_t$和前一时间步$t-1$的隐状态$\\mathbf{H}_{t-1}$；\n",
    "1. 将拼接的结果送入带有激活函数$\\phi$的全连接层。\n",
    "   全连接层的输出是当前时间步$t$的隐状态$\\mathbf{H}_t$。\n",
    "   \n",
    "在本例中，模型参数是$\\mathbf{W}_{xh}$和$\\mathbf{W}_{hh}$的拼接，\n",
    "以及$\\mathbf{b}_h$的偏置。当前时间步$t$的隐状态$\\mathbf{H}_t$\n",
    "将参与计算下一时间步$t+1$的隐状态$\\mathbf{H}_{t+1}$。而且$\\mathbf{H}_t$还将送入全连接输出层，用于计算当前时间步$t$的输出$\\mathbf{O}_t$。\n",
    "\n",
    "![具有隐状态的循环神经网络](./rnn.svg)\n",
    ":label:`fig_rnn`\n",
    "\n",
    "我们刚才提到，隐状态中$\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh}$的计算，\n",
    "相当于$\\mathbf{X}_t$和$\\mathbf{H}_{t-1}$的拼接\n",
    "与$\\mathbf{W}_{xh}$和$\\mathbf{W}_{hh}$的拼接的矩阵乘法。\n",
    "虽然这个性质可以通过数学证明，\n",
    "但在下面我们使用一个简单的代码来说明一下。\n",
    "首先，我们定义矩阵`X`、`W_xh`、`H`和`W_hh`，\n",
    "它们的形状分别为$(3，1)$、$(1，4)$、$(3，4)$和$(4，4)$。\n",
    "分别将`X`乘以`W_xh`，将`H`乘以`W_hh`，\n",
    "然后将这两个乘法相加，我们得到一个形状为$(3，4)$的矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99658d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd02c6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7097,  0.2873,  0.0954, -0.5006],\n",
       "        [ 1.0670, -2.3663, -1.4578,  1.6446],\n",
       "        [ 0.2599,  0.9628, -2.0666,  0.5380]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, W_xh = torch.normal(0, 1, (3, 1)), torch.normal(0, 1, (1, 4))\n",
    "H, W_hh = torch.normal(0, 1, (3, 4)), torch.normal(0, 1, (4, 4))\n",
    "torch.matmul(X,W_xh) + torch.matmul(H, W_hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec93a998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7097,  0.2873,  0.0954, -0.5006],\n",
       "        [ 1.0670, -2.3663, -1.4578,  1.6446],\n",
       "        [ 0.2599,  0.9628, -2.0666,  0.5380]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(torch.cat((X, H), 1), torch.cat((W_xh, W_hh), 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486173a1",
   "metadata": {},
   "source": [
    "## 基于循环神经网络的字符级语言模型\n",
    "在语言模型中，我们的目标是根据过去的和当前的词元预测下一个词元，因此我们将原始序列移位一个词元作为标签。接下来，我们看一下如何使用循环神经网络来构建语言模型。设小批量大小为1，批量中的那个文本序列为“machine”。为了简化后续部分的训练，我们考虑使用字符级语言模型（character-level language model），将文本词元化为字符而不是单词。下图演示了通过基于字符级语言模型的循环神经网络，使用当前的和之前的字符预测下一个字符。\n",
    "\n",
    "![基于循环神经网络的字符级语言模型：输入序列和标签序列分别为“machin”和“achine”](./rnn-train.svg)\n",
    ":label:`fig_rnn_train`\n",
    "\n",
    "在训练过程中，我们对每个时间步的输出层的输出进行softmax操作，然后利用交叉熵损失计算模型输出和标签之间的误差。由于隐藏层中隐状态的循环计算， :numref:`fig_rnn_train`中的第$3$个时间步的输出$\\mathbf{O}_3$由文本序列“m”、“a”和“c”确定。由于训练数据中这个文本序列的下一个字符是“h”，因此第$3$个时间步的损失将取决于下一个字符的概率分布，而下一个字符是基于特征序列“m”、“a”、“c”和这个时间步的标签“h”生成的。\n",
    "\n",
    "在实践中，我们使用的批量大小为$n>1$，每个词元都由一个$d$维向量表示。因此，在时间步$t$输入$\\mathbf X_t$将是一个$n\\times d$矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458ecf00",
   "metadata": {},
   "source": [
    "## 困惑度（Perplexity）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dde5cc9",
   "metadata": {},
   "source": [
    "最后，让我们讨论如何度量语言模型的质量，这将在后续部分中用于评估基于循环神经网络的模型。我们可以通过一个序列中所有的$n$个词元的交叉熵损失的平均值来衡量：\n",
    "\n",
    "$$\\frac{1}{n} \\sum_{t=1}^n -\\log P(x_t \\mid x_{t-1}, \\ldots, x_1),$$\n",
    ":eqlabel:`eq_avg_ce_for_lm`\n",
    "\n",
    "其中$P$由语言模型给出，$x_t$是在时间步$t$从该序列中观察到的实际词元。这使得不同长度的文档的性能具有了可比性。由于历史原因，自然语言处理的科学家更喜欢使用一个叫做*困惑度*（perplexity）的量。简而言之，它是 :eqref:`eq_avg_ce_for_lm`的指数：\n",
    "\n",
    "$$\\exp\\left(-\\frac{1}{n} \\sum_{t=1}^n \\log P(x_t \\mid x_{t-1}, \\ldots, x_1)\\right).$$\n",
    "\n",
    "在最好的情况下，模型总是完美地估计标签词元的概率为1。在这种情况下，模型的困惑度为1。在最坏的情况下，模型总是预测标签词元的概率为0。在这种情况下，困惑度是正无穷大。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813a18e0",
   "metadata": {},
   "source": [
    "## 小结\n",
    "* 循环神经网络的输出取决于当下输入和前一时刻的隐变量\n",
    "* 应用到语言模型中，循环神经网络根据当前词预测下一时刻的词\n",
    "* 使用困惑度来衡量语言模型的好坏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c61b59d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
